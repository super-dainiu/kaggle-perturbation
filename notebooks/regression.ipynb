{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('/home/yz979/code/kaggle-perturbation/')\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '5'\n",
    "\n",
    "train_path = 'data/de_train.parquet'\n",
    "submit_path = 'data/id_map.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import anndata\n",
    "import networkx as nx\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Any, Union, Optional\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv, GATConv, SAGEConv\n",
    "from torch_geometric.nn.norm import BatchNorm, GraphNorm\n",
    "from torch.utils.data import Dataset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularGraphDataset(Dataset):\n",
    "    def __init__(self, df, cond_cols, target_cols, perturb_key, edge_index=None, edge_attr=None, scaler=None):\n",
    "        self.df = df\n",
    "        self.cond_cols = cond_cols\n",
    "        self.perturb_key = perturb_key\n",
    "        self.target_cols = target_cols\n",
    "        self.cat_dict = {}\n",
    "        self.edge_index = edge_index\n",
    "        self.edge_attr = edge_attr\n",
    "        self.scaler = scaler\n",
    "\n",
    "        for cond_col in cond_cols:\n",
    "            unique_values = df[cond_col].unique()\n",
    "            self.cat_dict[cond_col] = {val: i for i, val in enumerate(unique_values)}\n",
    "        \n",
    "        unique_values = df[perturb_key].unique()\n",
    "        self.cat_dict[perturb_key] = {val: i for i, val in enumerate(unique_values)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        ret_dict = {}\n",
    "        for cond_col in self.cond_cols:\n",
    "            ret_dict[cond_col] = torch.tensor(self.cat_dict[cond_col][row[cond_col]])\n",
    "        ret_dict[self.perturb_key] = torch.tensor(self.cat_dict[self.perturb_key][row[self.perturb_key]])\n",
    "        target = torch.tensor(row[self.target_cols].values.astype(np.float32)) if self.target_cols else torch.tensor([0])\n",
    "        return Data(x=None, edge_index=self.edge_index, edge_attr=self.edge_attr, **ret_dict), target\n",
    "\n",
    "    def set_graph(self, edge_index, edge_attr):\n",
    "        self.edge_index = edge_index\n",
    "        self.edge_attr = edge_attr\n",
    "\n",
    "    def encode(self, val, col):\n",
    "        return self.cat_dict[col][val]\n",
    "\n",
    "    def decode(self, val, col):\n",
    "        return list(self.cat_dict[col].keys())[val]\n",
    "\n",
    "    def vocab_size(self, col):\n",
    "        return len(self.cat_dict[col])\n",
    "\n",
    "def validate_config(dataset, config):\n",
    "    config['perturb_key'] = dataset.perturb_key\n",
    "    for cond_col in dataset.cond_cols:\n",
    "        config[f'num_{cond_col}s'] = dataset.vocab_size(cond_col)\n",
    "    config[f'num_{dataset.perturb_key}s'] = dataset.vocab_size(dataset.perturb_key)\n",
    "    config['num_targets'] = len(dataset.target_cols)\n",
    "    config['conditions'] = dataset.cond_cols\n",
    "    return config\n",
    "\n",
    "def load_dataset(\n",
    "    path,\n",
    "    cond_cols = ['cell_type'],\n",
    "    perturb_key = 'perturb',\n",
    "    col_map = {'cell_type': 'cell_type', 'sm_lincs_id': 'perturb'},\n",
    "    is_test = False,\n",
    "    scaler = None,\n",
    "    cat_dict = None,\n",
    "):\n",
    "    df = pd.read_parquet(path)\n",
    "    df = df.rename(columns=col_map)\n",
    "    target_cols = df.iloc[:, 5:].columns.tolist() if not is_test else None\n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "        df.iloc[:, 5:] = scaler.fit_transform(df.iloc[:, 5:])\n",
    "    ds = TabularGraphDataset(df, cond_cols, target_cols, perturb_key, scaler=scaler)\n",
    "    if cat_dict:\n",
    "        ds.cat_dict = cat_dict\n",
    "    return ds\n",
    "\n",
    "@dataclass\n",
    "class GeneSimNetwork():\n",
    "    G: nx.DiGraph\n",
    "    edge_index: torch.Tensor\n",
    "    edge_weight: torch.Tensor\n",
    "    \n",
    "    @classmethod\n",
    "    def from_edges(\n",
    "        cls,\n",
    "        edge_list: pd.DataFrame,\n",
    "        gene_list: List,\n",
    "        node_map: Dict[str, int],\n",
    "        ) -> \"GeneSimNetwork\":\n",
    "        \"\"\"\n",
    "        Generate gene similarity network from edge list\n",
    "\n",
    "        Args:\n",
    "            edge_list (pd.DataFrame): edge list of the network\n",
    "            gene_list (list): list of gene names\n",
    "            node_map (dict): dictionary mapping gene names to node indices\n",
    "\n",
    "        Returns:\n",
    "            GeneSimNetwork: gene similarity network\n",
    "        \"\"\"\n",
    "        G = nx.from_pandas_edgelist(edge_list, source='source',\n",
    "                  target='target', edge_attr=['importance'],\n",
    "                  create_using=nx.Graph())\n",
    "        for n in gene_list:\n",
    "            if n not in G.nodes():\n",
    "                G.add_node(n)\n",
    "\n",
    "        # Remove nodes not in gene_list\n",
    "        G.remove_nodes_from([n for n in G.nodes() if n not in gene_list])\n",
    "\n",
    "        # Remove duplicate edges\n",
    "        G.remove_edges_from(nx.selfloop_edges(G))\n",
    "\n",
    "        edge_index_ = [(node_map[e[0]], node_map[e[1]]) for e in G.edges]\n",
    "\n",
    "        edge_index = torch.tensor(edge_index_, dtype=torch.long).T\n",
    "        \n",
    "        edge_attr = nx.get_edge_attributes(G, 'importance') \n",
    "        importance = np.array([edge_attr[e] for e in G.edges])\n",
    "        edge_weight = torch.Tensor(importance)\n",
    "        \n",
    "        return cls(G, edge_index, edge_weight)\n",
    "\n",
    "def load_gene_network(path, gene_list, node_map, threshold=0.5):\n",
    "    edge_list = pd.read_csv(path)\n",
    "    print('Number of edges before filtering: ', len(edge_list))\n",
    "    edge_list = edge_list[edge_list['importance'] > threshold]\n",
    "    print('Number of edges after filtering: ', len(edge_list))\n",
    "    return GeneSimNetwork.from_edges(edge_list, gene_list, node_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mrrmse_loss(predicted, actual):\n",
    "    squared_error = (predicted - actual) ** 2\n",
    "    rowwise_mse = torch.mean(squared_error, dim=1)\n",
    "    rowwise_rmse = torch.sqrt(rowwise_mse)\n",
    "    mrrmse = torch.mean(rowwise_rmse)\n",
    "    return mrrmse\n",
    "\n",
    "def train_epoch(model, optimizer, dataloader, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for i, (data, target) in enumerate(dataloader):\n",
    "        data.to(device)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(**data.to_dict())\n",
    "        loss = mrrmse_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def validate_epoch(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, target) in enumerate(dataloader):\n",
    "            data.to(device)\n",
    "            target = target.to(device)\n",
    "            output = model(**data.to_dict())\n",
    "            loss = mrrmse_loss(output, target)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionEncoder(nn.Module):\n",
    "    def __init__(self, config, condition):\n",
    "        super(ConditionEncoder, self).__init__()\n",
    "        self.config = config\n",
    "        self.condition = condition\n",
    "        self.embeddings = nn.Embedding(self.config[f'num_{self.condition}s'], self.config['embed_size'])\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(self.config['embed_size'], self.config['hidden_size']),\n",
    "            nn.BatchNorm1d(self.config['hidden_size']) if self.config.get('batch_norm') else nn.Identity(),\n",
    "            nn.Dropout(self.config['dropout']),\n",
    "        )\n",
    "\n",
    "    def set_embedding(self, embedding, freeze=True):\n",
    "        self.embeddings = nn.Embedding.from_pretrained(embedding, freeze=freeze)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(self.embeddings(x))\n",
    "\n",
    "class PerturbationEncoder(nn.Module):\n",
    "    def __init__(self, config, condition):\n",
    "        super(PerturbationEncoder, self).__init__()\n",
    "        raise NotImplementedError\n",
    "\n",
    "class CellEncoder(nn.Module):\n",
    "    perturb_encoder_cls = ConditionEncoder  # PerturbationEncoder is not implemented yet (TODO)\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(CellEncoder, self).__init__()\n",
    "        self.config = config\n",
    "        self.cell_embedding = nn.Parameter(torch.empty(1, self.config['num_targets'], self.config['embed_size']))  # replace with better embedding\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(self.config['embed_size'], self.config['hidden_size']),\n",
    "            nn.LayerNorm(self.config['hidden_size']) if self.config.get('layer_norm') else nn.Identity(),\n",
    "            nn.Dropout(self.config['dropout']),\n",
    "        )\n",
    "        for condition in self.config['conditions']:\n",
    "            setattr(self, f'{condition}_encoder', ConditionEncoder(self.config, condition))\n",
    "        self.perturb_encoder = self.perturb_encoder_cls(self.config, config['perturb_key'])\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.cell_embedding)\n",
    "\n",
    "    def set_cell_embedding(self, cell_embedding, freeze=True):\n",
    "        self.cell_embedding = nn.Parameter(cell_embedding, requires_grad=not freeze)\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        cell_embedding = self.cell_embedding\n",
    "        condition_embedding = torch.cat([\n",
    "            getattr(self, f'{condition}_encoder')(kwargs[condition]).unsqueeze(1)\n",
    "            for condition in self.config['conditions']\n",
    "            ], dim=1)\n",
    "        condition_embedding = condition_embedding.sum(dim=1, keepdim=True)\n",
    "        perturb_embedding = self.perturb_encoder(kwargs[self.config['perturb_key']]).unsqueeze(1)\n",
    "        return self.encoder(cell_embedding), condition_embedding, perturb_embedding\n",
    "\n",
    "class ValueDecoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(ValueDecoder, self).__init__()\n",
    "        self.config = config\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.config['hidden_size'], self.config['hidden_size'] * 2),\n",
    "            BatchNorm(self.config['num_targets']) if self.config.get('batch_norm') else nn.Identity(),\n",
    "            nn.Dropout(self.config['dropout']),\n",
    "            config['activation'],\n",
    "            nn.Linear(self.config['hidden_size'] * 2, self.config['hidden_size']),\n",
    "            BatchNorm(self.config['num_targets']) if self.config.get('batch_norm') else nn.Identity(),\n",
    "            nn.Dropout(self.config['dropout']),\n",
    "            config['activation'],\n",
    "            nn.Linear(self.config['hidden_size'], 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(x).squeeze(-1)\n",
    "\n",
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.config = config\n",
    "        self.graph_conv_1 = GCNConv(self.config['hidden_size'], self.config['hidden_size'])\n",
    "        self.graph_conv_2 = GCNConv(self.config['hidden_size'], self.config['hidden_size'])\n",
    "        self.dropout = nn.Dropout(self.config['dropout'])\n",
    "        self.act_fn = config['activation']\n",
    "        \n",
    "        self.batch_norm_1 = BatchNorm(self.config['num_targets']) if self.config.get('batch_norm') else nn.Identity()\n",
    "        self.batch_norm_2 = BatchNorm(self.config['num_targets']) if self.config.get('batch_norm') else nn.Identity()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None):\n",
    "        res = x\n",
    "        x = self.batch_norm_1(self.dropout(self.act_fn(self.graph_conv_1(x, edge_index, edge_attr))))\n",
    "        x = self.batch_norm_2(self.dropout(self.act_fn(self.graph_conv_2(x, edge_index, edge_attr))))\n",
    "        return x + res\n",
    "\n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(TransformerLayer, self).__init__()\n",
    "        self.config = config\n",
    "        self.attn = nn.MultiheadAttention(self.config['hidden_size'], self.config['num_heads'])\n",
    "        self.layer_norm_1 = nn.LayerNorm(self.config['hidden_size'])\n",
    "        \n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(self.config['hidden_size'], self.config['hidden_size'] * 4),\n",
    "            nn.Dropout(self.config['dropout']),\n",
    "            nn.GELU(), \n",
    "            nn.Linear(self.config['hidden_size'] * 4, self.config['hidden_size']),\n",
    "        )\n",
    "        self.layer_norm_2 = nn.LayerNorm(self.config['hidden_size'])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.attn(x, x, x)[0] + x\n",
    "        x = self.layer_norm_1(x)\n",
    "        x = self.ffn(x) + x\n",
    "        x = self.layer_norm_2(x)\n",
    "        return x\n",
    "\n",
    "class DERegressor(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(DERegressor, self).__init__()\n",
    "        self.config = config\n",
    "        self.cell_encoder = CellEncoder(self.config)\n",
    "        self.value_decoder = ValueDecoder(self.config)\n",
    "        \n",
    "        self.w_x = nn.Parameter(torch.empty(self.config['num_targets'], 1))\n",
    "        self.w_c = nn.Parameter(torch.empty(1, self.config['hidden_size']))\n",
    "        self.w_p = nn.Parameter(torch.empty(1, self.config['hidden_size']))\n",
    "\n",
    "        self.edge_index = nn.Parameter(None, requires_grad=False)\n",
    "        self.edge_attr = nn.Parameter(None, requires_grad=False)\n",
    "        if self.config['model_type'] == 'gcn':\n",
    "            self.gcn_layers = nn.ModuleList([GCNLayer(self.config) for _ in range(self.config['num_layers'])])\n",
    "        elif self.config['model_type'] == 'transformer':\n",
    "            self.transformer_layers = nn.ModuleList([TransformerLayer(self.config) for _ in range(self.config['num_layers'])])\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.w_x)\n",
    "        nn.init.xavier_uniform_(self.w_c)\n",
    "        nn.init.xavier_uniform_(self.w_p)\n",
    "    \n",
    "    def forward(self, **kwargs):\n",
    "        x, c, p = self.cell_encoder(**kwargs)\n",
    "        x = x * self.w_x + c * self.w_c + p * self.w_p\n",
    "        if self.config['model_type'] == 'gcn':\n",
    "            for gcn_layer in self.gcn_layers:\n",
    "                x = gcn_layer(x, self.edge_index, self.edge_attr)\n",
    "        elif self.config['model_type'] == 'transformer':\n",
    "            for transformer_layer in self.transformer_layers:\n",
    "                x = transformer_layer(x)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return self.value_decoder(x)\n",
    "\n",
    "    def set_cell_embedding(self, cell_embedding, freeze=True):\n",
    "        self.cell_encoder.set_cell_embedding(cell_embedding, freeze=freeze)\n",
    "\n",
    "    def set_condition_embedding(self, condition, condition_embedding, freeze=True):\n",
    "        getattr(self.cell_encoder, f'{condition}_encoder').set_embedding(condition_embedding, freeze=freeze)\n",
    "    \n",
    "    def set_graph(self, edge_index, edge_attr):\n",
    "        self.edge_index = nn.Parameter(edge_index, requires_grad=False)\n",
    "        self.edge_attr = nn.Parameter(edge_attr, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges before filtering:  15787508\n",
      "Number of edges after filtering:  1784693\n",
      "Number of nodes:  18211\n",
      "Number of edges:  1750737\n",
      "Number of isolated nodes:  13770\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(train_path)\n",
    "submission = load_dataset(submit_path, is_test=True, cat_dict=dataset.cat_dict, scaler=dataset.scaler)\n",
    "\n",
    "gene_list = dataset.df.columns[5:].tolist()\n",
    "node_map = {gene: i for i, gene in enumerate(gene_list)}\n",
    "gene_sim_network = load_gene_network('data/grn/coexpression.csv', gene_list, node_map, threshold=0.3)\n",
    "\n",
    "# statistics of gene similarity network\n",
    "print('Number of nodes: ', gene_sim_network.G.number_of_nodes())\n",
    "print('Number of edges: ', gene_sim_network.G.number_of_edges())\n",
    "print('Number of isolated nodes: ', len(list(nx.isolates(gene_sim_network.G))))\n",
    "# degree distribution\n",
    "degrees = [d for n, d in gene_sim_network.G.degree()]\n",
    "plt.hist(degrees, bins=100)\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    'hidden_size': 256,\n",
    "    'embed_size': 128,\n",
    "    'activation': nn.LeakyReLU(),\n",
    "    'num_layers': 2,\n",
    "    'num_heads': 8,\n",
    "    'batch_norm': True,\n",
    "    'layer_norm': False,\n",
    "    'dropout': 0.2,\n",
    "    'model_type': 'gcn',\n",
    "}\n",
    "model_config = validate_config(dataset, model_config)\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [int(len(dataset) * 0.9), len(dataset) - int(len(dataset) * 0.9)], generator=torch.Generator().manual_seed(42))\n",
    "train_loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "geneformer_embeddings = torch.load('models/geneformer_embeddings_.pt')\n",
    "chemberta_embeddings = torch.load('models/chemberta_embeddings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss: 0.5920405533019598 val rmse: 0.6242292411625385\n",
      "tensor(5.2211, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7288, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6920, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Saving model with val loss 0.6242292411625385\n",
      "Epoch 1 train loss: 0.600231226001467 val rmse: 0.5761245265603065\n",
      "tensor(5.1551, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7850, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6778, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Saving model with val loss 0.5761245265603065\n",
      "Epoch 2 train loss: 0.6083049450988893 val rmse: 0.6370275057852268\n",
      "tensor(5.0998, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7656, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7041, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 3 train loss: 0.6019214546525633 val rmse: 0.6459718272089958\n",
      "tensor(5.1647, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8045, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6863, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 4 train loss: 0.5756713074523133 val rmse: 0.5968684554100037\n",
      "tensor(5.1602, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8246, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6665, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 5 train loss: 0.5771844238042831 val rmse: 0.6925537064671516\n",
      "tensor(5.2125, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7807, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6968, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 6 train loss: 0.581923892745724 val rmse: 0.5699619874358177\n",
      "tensor(5.2283, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7726, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6996, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Saving model with val loss 0.5699619874358177\n",
      "Epoch 7 train loss: 0.5759667678699865 val rmse: 0.5782323889434338\n",
      "tensor(5.2193, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7740, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7044, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 8 train loss: 0.5887095757893154 val rmse: 0.6306432709097862\n",
      "tensor(5.1783, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7982, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6958, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 9 train loss: 0.5710830580104481 val rmse: 0.7382725700736046\n",
      "tensor(5.2295, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7941, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6949, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 10 train loss: 0.5661451419839612 val rmse: 0.6943977698683739\n",
      "tensor(5.2260, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7720, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7217, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 11 train loss: 0.5651222066832827 val rmse: 0.5604158118367195\n",
      "tensor(5.3048, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7694, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7106, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Saving model with val loss 0.5604158118367195\n",
      "Epoch 12 train loss: 0.5603077071053642 val rmse: 0.6159688159823418\n",
      "tensor(5.3053, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7824, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7002, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 13 train loss: 0.5602690676203022 val rmse: 0.6441167891025543\n",
      "tensor(5.3222, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7839, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6975, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 14 train loss: 0.5573696039326779 val rmse: 0.6807859614491463\n",
      "tensor(5.3060, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7823, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6966, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 15 train loss: 0.5578605039166166 val rmse: 0.5964159965515137\n",
      "tensor(5.3233, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7805, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6930, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 16 train loss: 0.5580644400475862 val rmse: 0.6203451566398144\n",
      "tensor(5.3171, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7855, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6905, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 17 train loss: 0.5557452956964444 val rmse: 0.6056608781218529\n",
      "tensor(5.3223, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7880, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6820, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 18 train loss: 0.5568531825170888 val rmse: 0.6140613630414009\n",
      "tensor(5.3253, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7860, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6806, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 19 train loss: 0.5528887643829569 val rmse: 0.619441457092762\n",
      "tensor(5.3139, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7827, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6817, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 20 train loss: 0.5610234996715149 val rmse: 0.6182232685387135\n",
      "tensor(5.3276, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7911, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6744, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 21 train loss: 0.5562428743034215 val rmse: 0.6275946870446205\n",
      "tensor(5.3463, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7884, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6738, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 22 train loss: 0.5529469704473173 val rmse: 0.6583824902772903\n",
      "tensor(5.3327, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7877, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6714, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 23 train loss: 0.5546769623632555 val rmse: 0.5927958898246288\n",
      "tensor(5.3435, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7886, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6704, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 24 train loss: 0.5534323655165635 val rmse: 0.6467504948377609\n",
      "tensor(5.3491, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7901, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6659, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 25 train loss: 0.5546759448268197 val rmse: 0.6174666881561279\n",
      "tensor(5.3582, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7883, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6637, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 26 train loss: 0.5554777250274435 val rmse: 0.6297102011740208\n",
      "tensor(5.3421, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7887, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6567, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 27 train loss: 0.5532984065932113 val rmse: 0.6083631701767445\n",
      "tensor(5.3768, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7919, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6556, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 28 train loss: 0.5619225020145441 val rmse: 0.5956147313117981\n",
      "tensor(5.3653, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7766, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6691, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 29 train loss: 0.563834389889395 val rmse: 0.715871199965477\n",
      "tensor(5.3561, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8134, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6455, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 30 train loss: 0.5622574644429343 val rmse: 0.6257830783724785\n",
      "tensor(5.3886, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8043, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6448, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 31 train loss: 0.5735274456538163 val rmse: 0.5691637806594372\n",
      "tensor(5.4845, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8216, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6175, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 32 train loss: 0.5713368096908966 val rmse: 0.7875657752156258\n",
      "tensor(5.4388, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8203, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6244, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 33 train loss: 0.5798955851948107 val rmse: 0.5540290363132954\n",
      "tensor(5.5015, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8663, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6051, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Saving model with val loss 0.5540290363132954\n",
      "Epoch 34 train loss: 0.5681748963021612 val rmse: 0.6991478689014912\n",
      "tensor(5.5075, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8284, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6356, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 35 train loss: 0.570038021191374 val rmse: 0.68296068161726\n",
      "tensor(5.6188, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8500, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6189, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 36 train loss: 0.5734093909139757 val rmse: 0.6777738146483898\n",
      "tensor(5.6532, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8389, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6180, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 37 train loss: 0.5746882586897194 val rmse: 0.5981766059994698\n",
      "tensor(5.7529, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8302, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6314, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 38 train loss: 0.5892641387589566 val rmse: 0.5659088268876076\n",
      "tensor(5.7843, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8271, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6850, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 39 train loss: 0.5777276560857698 val rmse: 0.5681490004062653\n",
      "tensor(5.6471, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8195, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6893, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 40 train loss: 0.5797621135587816 val rmse: 0.6266291067004204\n",
      "tensor(5.7747, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8618, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6596, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 41 train loss: 0.5840372640591163 val rmse: 0.5857774280011654\n",
      "tensor(5.8002, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8479, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6574, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 42 train loss: 0.5692436873138725 val rmse: 0.7527391500771046\n",
      "tensor(5.8863, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8531, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6469, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 43 train loss: 0.5677823101158266 val rmse: 0.7215948030352592\n",
      "tensor(5.8865, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8468, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6495, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 44 train loss: 0.5756642750718377 val rmse: 0.6564673483371735\n",
      "tensor(5.9702, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8844, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6275, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 45 train loss: 0.5753260821878136 val rmse: 0.6457747183740139\n",
      "tensor(6.0300, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8827, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6138, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 46 train loss: 0.573166265309631 val rmse: 0.703211922198534\n",
      "tensor(6.1039, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.9139, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.5997, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 47 train loss: 0.5659011989444881 val rmse: 0.6770961359143257\n",
      "tensor(6.1823, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.9140, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6025, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 48 train loss: 0.5782247433414707 val rmse: 0.6370309218764305\n",
      "tensor(6.1724, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.9175, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6001, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 49 train loss: 0.5599783210011272 val rmse: 0.6946181729435921\n",
      "tensor(6.1630, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8882, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6133, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 50 train loss: 0.5663797921174533 val rmse: 0.6325722262263298\n",
      "tensor(6.1737, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8980, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6039, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 51 train loss: 0.5629693465186404 val rmse: 0.5938249602913857\n",
      "tensor(6.2124, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8873, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6010, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 52 train loss: 0.5571460934815469 val rmse: 0.652911052107811\n",
      "tensor(6.2559, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8797, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6064, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 53 train loss: 0.5517320743241867 val rmse: 0.6141975149512291\n",
      "tensor(6.2901, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8739, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6032, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 54 train loss: 0.5533242370788153 val rmse: 0.6633671373128891\n",
      "tensor(6.2763, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8691, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6020, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 55 train loss: 0.5499372033329754 val rmse: 0.7091684527695179\n",
      "tensor(6.2463, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8644, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6047, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 56 train loss: 0.5561211418795895 val rmse: 0.6774463839828968\n",
      "tensor(6.2348, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8828, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.5919, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 57 train loss: 0.5551110475094287 val rmse: 0.6025576032698154\n",
      "tensor(6.2410, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8805, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.5898, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 58 train loss: 0.5464909637903238 val rmse: 0.6749076694250107\n",
      "tensor(6.2393, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8755, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.5938, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 59 train loss: 0.5506588944367 val rmse: 0.6578876562416553\n",
      "tensor(6.2302, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8753, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.5909, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 60 train loss: 0.5446788558325211 val rmse: 0.6022263094782829\n",
      "tensor(6.2252, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8760, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.5881, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 61 train loss: 0.5459082194349982 val rmse: 0.6399163529276848\n",
      "tensor(6.2370, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8779, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.5853, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 62 train loss: 0.5519030105758023 val rmse: 0.6204115375876427\n",
      "tensor(6.2370, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8798, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.5829, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 63 train loss: 0.5456605112397825 val rmse: 0.6198354586958885\n",
      "tensor(6.2329, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8805, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.5780, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 64 train loss: 0.5478924222580799 val rmse: 0.6457572467625141\n",
      "tensor(6.2309, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8839, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.5724, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 65 train loss: 0.5514870111818437 val rmse: 0.6848657093942165\n",
      "tensor(6.2672, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8766, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.5747, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 66 train loss: 0.5481137670092768 val rmse: 0.6285718381404877\n",
      "tensor(6.2648, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8788, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.5722, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 67 train loss: 0.5494696853996871 val rmse: 0.6735693663358688\n",
      "tensor(6.2319, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8852, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.5608, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 68 train loss: 0.5556537479936302 val rmse: 0.7549549005925655\n",
      "tensor(6.2442, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8851, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.5487, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 69 train loss: 0.5517928873176698 val rmse: 0.6645034067332745\n",
      "tensor(6.2808, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8929, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.5295, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 70 train loss: 0.5518080706333185 val rmse: 0.6665713265538216\n",
      "tensor(6.2907, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8826, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.5424, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 71 train loss: 0.5668230668290869 val rmse: 0.5838072784245014\n",
      "tensor(6.2555, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8687, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.5576, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 72 train loss: 0.5591968299893589 val rmse: 0.5677404031157494\n",
      "tensor(6.1978, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8639, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.5610, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 73 train loss: 0.5632802197685489 val rmse: 0.655986126512289\n",
      "tensor(6.4245, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8703, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.5544, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 74 train loss: 0.5674982332176977 val rmse: 0.6918377988040447\n",
      "tensor(6.3383, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8648, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.5838, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 75 train loss: 0.5764186871903283 val rmse: 0.5609175600111485\n",
      "tensor(6.3377, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8574, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.5765, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 76 train loss: 0.5631164972271238 val rmse: 0.7018937766551971\n",
      "tensor(6.3658, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8282, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6086, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 77 train loss: 0.5600344939278318 val rmse: 0.6431306973099709\n",
      "tensor(6.3965, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8368, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.5935, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 78 train loss: 0.5737233616701969 val rmse: 0.6759996525943279\n",
      "tensor(6.4873, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8922, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.5405, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 79 train loss: 0.562628328993723 val rmse: 0.5774711035192013\n",
      "tensor(6.5209, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8678, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.5500, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 80 train loss: 0.5604453783530694 val rmse: 0.6986109465360641\n",
      "tensor(6.6335, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8470, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.5560, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 81 train loss: 0.5677402812939185 val rmse: 0.602588914334774\n",
      "tensor(6.6484, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8477, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.5613, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 82 train loss: 0.5620532637679732 val rmse: 0.6472352519631386\n",
      "tensor(6.6772, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8470, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.5695, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 83 train loss: 0.5636026300006098 val rmse: 0.6911063641309738\n",
      "tensor(6.5669, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.9074, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.5121, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 84 train loss: 0.5700929548059191 val rmse: 0.7157955169677734\n",
      "tensor(6.6439, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.9354, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.4829, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 85 train loss: 0.5683766022905127 val rmse: 0.6659572571516037\n",
      "tensor(6.6731, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8943, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.5377, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 86 train loss: 0.5576641772474561 val rmse: 0.7470798194408417\n",
      "tensor(6.7084, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8848, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.5349, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 87 train loss: 0.5613750859901503 val rmse: 0.7851024940609932\n",
      "tensor(6.7009, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8744, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.5596, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 88 train loss: 0.5555867575979853 val rmse: 0.6122789308428764\n",
      "tensor(6.6970, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8919, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.5426, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 89 train loss: 0.5537695584746151 val rmse: 0.6852179504930973\n",
      "tensor(6.7592, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8809, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.5501, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 90 train loss: 0.5509394006682681 val rmse: 0.6577420309185982\n",
      "tensor(6.8275, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8801, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.5336, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 91 train loss: 0.54831607446268 val rmse: 0.6615655794739723\n",
      "tensor(6.8576, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8850, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.5308, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 92 train loss: 0.5488461604366055 val rmse: 0.6451867558062077\n",
      "tensor(6.8416, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8798, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.5212, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 93 train loss: 0.5522634532157477 val rmse: 0.7040118016302586\n",
      "tensor(6.8160, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.9031, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.5017, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 94 train loss: 0.5491122645991189 val rmse: 0.646811943501234\n",
      "tensor(6.8156, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.9158, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.4841, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 95 train loss: 0.5478723376215279 val rmse: 0.5924793854355812\n",
      "tensor(6.8183, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.9220, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.4744, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 96 train loss: 0.543261267148055 val rmse: 0.6538417488336563\n",
      "tensor(6.7945, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.9106, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.4823, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 97 train loss: 0.5386985455240522 val rmse: 0.6379969641566277\n",
      "tensor(6.7962, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.9060, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.4800, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 98 train loss: 0.5403747545047239 val rmse: 0.6307757049798965\n",
      "tensor(6.7973, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.9039, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.4798, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 99 train loss: 0.5437966598705812 val rmse: 0.6649885326623917\n",
      "tensor(6.7963, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.9061, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.4781, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "experiment_name = 'grn_model_4'\n",
    "\n",
    "# model = DERegressor(model_config)\n",
    "# model.w_p.data *= 2\n",
    "# model.w_x.data *= 5\n",
    "# model.set_cell_embedding(geneformer_embeddings, freeze=True)\n",
    "# model.set_condition_embedding('perturb', chemberta_embeddings, freeze=True)\n",
    "# model.set_graph(gene_sim_network.edge_index, gene_sim_network.edge_weight)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20, eta_min=1e-4)\n",
    "\n",
    "model = model.to(device)\n",
    "best_val_loss = np.inf\n",
    "for epoch in range(100):\n",
    "    train_loss = train_epoch(model, optimizer, train_loader, device)\n",
    "    val_loss = validate_epoch(model, val_loader, device)\n",
    "    print(f'Epoch {epoch} train loss: {train_loss} val rmse: {val_loss}')\n",
    "    scheduler.step()\n",
    "    print(torch.norm(model.w_x), torch.norm(model.w_c), torch.norm(model.w_p))\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        print(f'Saving model with val loss {val_loss}')\n",
    "        torch.save(model.state_dict(), f'models/{experiment_name}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DERegressor(model_config)\n",
    "model.set_graph(gene_sim_network.edge_index, gene_sim_network.edge_weight)\n",
    "model.load_state_dict(torch.load('models/grn_model_4.pt'))\n",
    "model = model.to(device)\n",
    "test_loader = DataLoader(submission, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1BG</th>\n",
       "      <th>A1BG-AS1</th>\n",
       "      <th>A2M</th>\n",
       "      <th>A2M-AS1</th>\n",
       "      <th>A2MP1</th>\n",
       "      <th>A4GALT</th>\n",
       "      <th>AAAS</th>\n",
       "      <th>AACS</th>\n",
       "      <th>AAGAB</th>\n",
       "      <th>AAK1</th>\n",
       "      <th>...</th>\n",
       "      <th>ZUP1</th>\n",
       "      <th>ZW10</th>\n",
       "      <th>ZWILCH</th>\n",
       "      <th>ZWINT</th>\n",
       "      <th>ZXDA</th>\n",
       "      <th>ZXDB</th>\n",
       "      <th>ZXDC</th>\n",
       "      <th>ZYG11B</th>\n",
       "      <th>ZYX</th>\n",
       "      <th>ZZEF1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.240026</td>\n",
       "      <td>0.227807</td>\n",
       "      <td>0.746552</td>\n",
       "      <td>0.894780</td>\n",
       "      <td>1.126657</td>\n",
       "      <td>0.589827</td>\n",
       "      <td>-0.048677</td>\n",
       "      <td>0.398204</td>\n",
       "      <td>-0.044358</td>\n",
       "      <td>0.049487</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.045393</td>\n",
       "      <td>0.156115</td>\n",
       "      <td>-0.069291</td>\n",
       "      <td>0.113238</td>\n",
       "      <td>0.501081</td>\n",
       "      <td>0.297127</td>\n",
       "      <td>0.357612</td>\n",
       "      <td>0.399764</td>\n",
       "      <td>-0.132893</td>\n",
       "      <td>0.093337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.072927</td>\n",
       "      <td>0.032801</td>\n",
       "      <td>-0.072529</td>\n",
       "      <td>-0.006588</td>\n",
       "      <td>0.179815</td>\n",
       "      <td>0.810256</td>\n",
       "      <td>-0.093700</td>\n",
       "      <td>0.088748</td>\n",
       "      <td>-0.166407</td>\n",
       "      <td>0.049487</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.076654</td>\n",
       "      <td>-0.043695</td>\n",
       "      <td>-0.069977</td>\n",
       "      <td>0.067024</td>\n",
       "      <td>0.232489</td>\n",
       "      <td>0.098192</td>\n",
       "      <td>0.099706</td>\n",
       "      <td>0.030713</td>\n",
       "      <td>-0.156500</td>\n",
       "      <td>-0.093992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.309864</td>\n",
       "      <td>0.147829</td>\n",
       "      <td>0.010692</td>\n",
       "      <td>0.099289</td>\n",
       "      <td>0.564468</td>\n",
       "      <td>2.080191</td>\n",
       "      <td>-0.073163</td>\n",
       "      <td>0.212586</td>\n",
       "      <td>-0.088305</td>\n",
       "      <td>0.049487</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034165</td>\n",
       "      <td>0.049200</td>\n",
       "      <td>0.078094</td>\n",
       "      <td>0.371850</td>\n",
       "      <td>0.460077</td>\n",
       "      <td>0.200574</td>\n",
       "      <td>0.139173</td>\n",
       "      <td>0.022223</td>\n",
       "      <td>-0.137697</td>\n",
       "      <td>-0.097500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.009941</td>\n",
       "      <td>0.045511</td>\n",
       "      <td>0.037016</td>\n",
       "      <td>0.114494</td>\n",
       "      <td>0.124525</td>\n",
       "      <td>0.155428</td>\n",
       "      <td>-0.090574</td>\n",
       "      <td>0.127393</td>\n",
       "      <td>-0.117340</td>\n",
       "      <td>0.049487</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.064411</td>\n",
       "      <td>-0.051217</td>\n",
       "      <td>-0.165009</td>\n",
       "      <td>-0.036282</td>\n",
       "      <td>0.180924</td>\n",
       "      <td>0.119955</td>\n",
       "      <td>0.143147</td>\n",
       "      <td>0.125363</td>\n",
       "      <td>-0.113686</td>\n",
       "      <td>-0.050949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.176469</td>\n",
       "      <td>0.113052</td>\n",
       "      <td>0.020233</td>\n",
       "      <td>0.183254</td>\n",
       "      <td>0.964298</td>\n",
       "      <td>0.493571</td>\n",
       "      <td>-0.074642</td>\n",
       "      <td>0.162649</td>\n",
       "      <td>-0.079091</td>\n",
       "      <td>0.049487</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.058868</td>\n",
       "      <td>-0.009430</td>\n",
       "      <td>-0.150087</td>\n",
       "      <td>0.044731</td>\n",
       "      <td>0.300185</td>\n",
       "      <td>0.170060</td>\n",
       "      <td>0.136977</td>\n",
       "      <td>0.202932</td>\n",
       "      <td>-0.069543</td>\n",
       "      <td>-0.016513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>0.122551</td>\n",
       "      <td>0.038499</td>\n",
       "      <td>0.065220</td>\n",
       "      <td>0.130564</td>\n",
       "      <td>1.315348</td>\n",
       "      <td>0.227477</td>\n",
       "      <td>-0.078619</td>\n",
       "      <td>0.100094</td>\n",
       "      <td>-0.151714</td>\n",
       "      <td>0.049487</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.110982</td>\n",
       "      <td>-0.079855</td>\n",
       "      <td>-0.134584</td>\n",
       "      <td>0.006962</td>\n",
       "      <td>0.303412</td>\n",
       "      <td>0.196349</td>\n",
       "      <td>0.160046</td>\n",
       "      <td>0.069739</td>\n",
       "      <td>0.060497</td>\n",
       "      <td>-0.108335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>0.167742</td>\n",
       "      <td>0.062615</td>\n",
       "      <td>-0.218584</td>\n",
       "      <td>0.190218</td>\n",
       "      <td>2.339814</td>\n",
       "      <td>0.330166</td>\n",
       "      <td>-0.061242</td>\n",
       "      <td>0.059215</td>\n",
       "      <td>-0.089956</td>\n",
       "      <td>0.049487</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.055675</td>\n",
       "      <td>-0.069091</td>\n",
       "      <td>-0.222568</td>\n",
       "      <td>-0.031470</td>\n",
       "      <td>0.392642</td>\n",
       "      <td>0.169633</td>\n",
       "      <td>0.086521</td>\n",
       "      <td>0.136514</td>\n",
       "      <td>-0.031644</td>\n",
       "      <td>-0.150636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>0.108062</td>\n",
       "      <td>-0.033660</td>\n",
       "      <td>0.019838</td>\n",
       "      <td>0.048147</td>\n",
       "      <td>1.788034</td>\n",
       "      <td>0.099609</td>\n",
       "      <td>-0.081659</td>\n",
       "      <td>0.024442</td>\n",
       "      <td>-0.194851</td>\n",
       "      <td>0.049487</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.105383</td>\n",
       "      <td>-0.194394</td>\n",
       "      <td>-0.159824</td>\n",
       "      <td>-0.055034</td>\n",
       "      <td>0.224868</td>\n",
       "      <td>0.135251</td>\n",
       "      <td>0.127380</td>\n",
       "      <td>-0.016388</td>\n",
       "      <td>0.017294</td>\n",
       "      <td>-0.150790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>7.201160</td>\n",
       "      <td>3.185865</td>\n",
       "      <td>-0.804578</td>\n",
       "      <td>7.592809</td>\n",
       "      <td>56.870285</td>\n",
       "      <td>9.276725</td>\n",
       "      <td>0.838207</td>\n",
       "      <td>1.213750</td>\n",
       "      <td>1.897484</td>\n",
       "      <td>0.049487</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144645</td>\n",
       "      <td>1.326430</td>\n",
       "      <td>-5.844079</td>\n",
       "      <td>0.389876</td>\n",
       "      <td>10.775970</td>\n",
       "      <td>2.513829</td>\n",
       "      <td>-3.267837</td>\n",
       "      <td>1.138314</td>\n",
       "      <td>-0.010672</td>\n",
       "      <td>-0.946597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>0.211887</td>\n",
       "      <td>0.062078</td>\n",
       "      <td>-0.205035</td>\n",
       "      <td>0.223782</td>\n",
       "      <td>2.520529</td>\n",
       "      <td>0.425253</td>\n",
       "      <td>-0.051588</td>\n",
       "      <td>0.086431</td>\n",
       "      <td>-0.119481</td>\n",
       "      <td>0.049487</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054856</td>\n",
       "      <td>-0.092057</td>\n",
       "      <td>-0.145577</td>\n",
       "      <td>0.036555</td>\n",
       "      <td>0.552074</td>\n",
       "      <td>0.206974</td>\n",
       "      <td>0.107009</td>\n",
       "      <td>0.095769</td>\n",
       "      <td>-0.016338</td>\n",
       "      <td>-0.162376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>255 rows  18211 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         A1BG  A1BG-AS1       A2M   A2M-AS1      A2MP1    A4GALT      AAAS  \\\n",
       "0    0.240026  0.227807  0.746552  0.894780   1.126657  0.589827 -0.048677   \n",
       "1    0.072927  0.032801 -0.072529 -0.006588   0.179815  0.810256 -0.093700   \n",
       "2    0.309864  0.147829  0.010692  0.099289   0.564468  2.080191 -0.073163   \n",
       "3    0.009941  0.045511  0.037016  0.114494   0.124525  0.155428 -0.090574   \n",
       "4    0.176469  0.113052  0.020233  0.183254   0.964298  0.493571 -0.074642   \n",
       "..        ...       ...       ...       ...        ...       ...       ...   \n",
       "250  0.122551  0.038499  0.065220  0.130564   1.315348  0.227477 -0.078619   \n",
       "251  0.167742  0.062615 -0.218584  0.190218   2.339814  0.330166 -0.061242   \n",
       "252  0.108062 -0.033660  0.019838  0.048147   1.788034  0.099609 -0.081659   \n",
       "253  7.201160  3.185865 -0.804578  7.592809  56.870285  9.276725  0.838207   \n",
       "254  0.211887  0.062078 -0.205035  0.223782   2.520529  0.425253 -0.051588   \n",
       "\n",
       "         AACS     AAGAB      AAK1  ...      ZUP1      ZW10    ZWILCH  \\\n",
       "0    0.398204 -0.044358  0.049487  ... -0.045393  0.156115 -0.069291   \n",
       "1    0.088748 -0.166407  0.049487  ... -0.076654 -0.043695 -0.069977   \n",
       "2    0.212586 -0.088305  0.049487  ... -0.034165  0.049200  0.078094   \n",
       "3    0.127393 -0.117340  0.049487  ... -0.064411 -0.051217 -0.165009   \n",
       "4    0.162649 -0.079091  0.049487  ... -0.058868 -0.009430 -0.150087   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "250  0.100094 -0.151714  0.049487  ... -0.110982 -0.079855 -0.134584   \n",
       "251  0.059215 -0.089956  0.049487  ... -0.055675 -0.069091 -0.222568   \n",
       "252  0.024442 -0.194851  0.049487  ... -0.105383 -0.194394 -0.159824   \n",
       "253  1.213750  1.897484  0.049487  ...  0.144645  1.326430 -5.844079   \n",
       "254  0.086431 -0.119481  0.049487  ... -0.054856 -0.092057 -0.145577   \n",
       "\n",
       "        ZWINT       ZXDA      ZXDB      ZXDC    ZYG11B       ZYX     ZZEF1  \n",
       "0    0.113238   0.501081  0.297127  0.357612  0.399764 -0.132893  0.093337  \n",
       "1    0.067024   0.232489  0.098192  0.099706  0.030713 -0.156500 -0.093992  \n",
       "2    0.371850   0.460077  0.200574  0.139173  0.022223 -0.137697 -0.097500  \n",
       "3   -0.036282   0.180924  0.119955  0.143147  0.125363 -0.113686 -0.050949  \n",
       "4    0.044731   0.300185  0.170060  0.136977  0.202932 -0.069543 -0.016513  \n",
       "..        ...        ...       ...       ...       ...       ...       ...  \n",
       "250  0.006962   0.303412  0.196349  0.160046  0.069739  0.060497 -0.108335  \n",
       "251 -0.031470   0.392642  0.169633  0.086521  0.136514 -0.031644 -0.150636  \n",
       "252 -0.055034   0.224868  0.135251  0.127380 -0.016388  0.017294 -0.150790  \n",
       "253  0.389876  10.775970  2.513829 -3.267837  1.138314 -0.010672 -0.946597  \n",
       "254  0.036555   0.552074  0.206974  0.107009  0.095769 -0.016338 -0.162376  \n",
       "\n",
       "[255 rows x 18211 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# submit\n",
    "model.eval()\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for i, (data, _) in enumerate(test_loader):\n",
    "        data.to(device)\n",
    "        output = model(**data.to_dict())\n",
    "        preds.append(output.cpu().numpy())\n",
    "\n",
    "preds = np.concatenate(preds)\n",
    "preds = dataset.scaler.inverse_transform(preds)\n",
    "preds = pd.DataFrame(preds, columns=dataset.target_cols)\n",
    "preds.to_csv('data/submission_7.csv', index_label='id')\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cell_type</th>\n",
       "      <th>sm_name</th>\n",
       "      <th>perturb</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>B cells</td>\n",
       "      <td>Vorinostat</td>\n",
       "      <td>LSM-3828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>Myeloid cells</td>\n",
       "      <td>Vorinostat</td>\n",
       "      <td>LSM-3828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         cell_type     sm_name   perturb\n",
       "id                                      \n",
       "126        B cells  Vorinostat  LSM-3828\n",
       "253  Myeloid cells  Vorinostat  LSM-3828"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# submission.df.perturb == submission.decode(15, 'perturb')\n",
    "find = submission.df[submission.df.perturb == submission.decode(15, 'perturb')]\n",
    "find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1BG</th>\n",
       "      <th>A1BG-AS1</th>\n",
       "      <th>A2M</th>\n",
       "      <th>A2M-AS1</th>\n",
       "      <th>A2MP1</th>\n",
       "      <th>A4GALT</th>\n",
       "      <th>AAAS</th>\n",
       "      <th>AACS</th>\n",
       "      <th>AAGAB</th>\n",
       "      <th>AAK1</th>\n",
       "      <th>...</th>\n",
       "      <th>ZUP1</th>\n",
       "      <th>ZW10</th>\n",
       "      <th>ZWILCH</th>\n",
       "      <th>ZWINT</th>\n",
       "      <th>ZXDA</th>\n",
       "      <th>ZXDB</th>\n",
       "      <th>ZXDC</th>\n",
       "      <th>ZYG11B</th>\n",
       "      <th>ZYX</th>\n",
       "      <th>ZZEF1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.187844</td>\n",
       "      <td>0.343249</td>\n",
       "      <td>-1.367830</td>\n",
       "      <td>-0.624914</td>\n",
       "      <td>2.002171</td>\n",
       "      <td>0.418658</td>\n",
       "      <td>0.221087</td>\n",
       "      <td>0.227081</td>\n",
       "      <td>2.394212</td>\n",
       "      <td>0.087044</td>\n",
       "      <td>...</td>\n",
       "      <td>0.129910</td>\n",
       "      <td>0.221945</td>\n",
       "      <td>-1.98317</td>\n",
       "      <td>-0.009505</td>\n",
       "      <td>0.102313</td>\n",
       "      <td>0.055104</td>\n",
       "      <td>-0.459282</td>\n",
       "      <td>0.630109</td>\n",
       "      <td>-0.268355</td>\n",
       "      <td>-0.082340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>2.827626</td>\n",
       "      <td>1.024067</td>\n",
       "      <td>-4.059803</td>\n",
       "      <td>2.349225</td>\n",
       "      <td>17.326069</td>\n",
       "      <td>4.039629</td>\n",
       "      <td>0.623714</td>\n",
       "      <td>0.180134</td>\n",
       "      <td>2.420457</td>\n",
       "      <td>0.087822</td>\n",
       "      <td>...</td>\n",
       "      <td>0.073284</td>\n",
       "      <td>0.612915</td>\n",
       "      <td>-3.97044</td>\n",
       "      <td>-0.511649</td>\n",
       "      <td>1.468711</td>\n",
       "      <td>-1.123937</td>\n",
       "      <td>-1.454470</td>\n",
       "      <td>0.800975</td>\n",
       "      <td>-0.220075</td>\n",
       "      <td>-0.028759</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows  18211 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         A1BG  A1BG-AS1       A2M   A2M-AS1      A2MP1    A4GALT      AAAS  \\\n",
       "126  0.187844  0.343249 -1.367830 -0.624914   2.002171  0.418658  0.221087   \n",
       "253  2.827626  1.024067 -4.059803  2.349225  17.326069  4.039629  0.623714   \n",
       "\n",
       "         AACS     AAGAB      AAK1  ...      ZUP1      ZW10   ZWILCH     ZWINT  \\\n",
       "126  0.227081  2.394212  0.087044  ...  0.129910  0.221945 -1.98317 -0.009505   \n",
       "253  0.180134  2.420457  0.087822  ...  0.073284  0.612915 -3.97044 -0.511649   \n",
       "\n",
       "         ZXDA      ZXDB      ZXDC    ZYG11B       ZYX     ZZEF1  \n",
       "126  0.102313  0.055104 -0.459282  0.630109 -0.268355 -0.082340  \n",
       "253  1.468711 -1.123937 -1.454470  0.800975 -0.220075 -0.028759  \n",
       "\n",
       "[2 rows x 18211 columns]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.iloc[find.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>A1BG</th>\n",
       "      <th>A1BG-AS1</th>\n",
       "      <th>A2M</th>\n",
       "      <th>A2M-AS1</th>\n",
       "      <th>A2MP1</th>\n",
       "      <th>A4GALT</th>\n",
       "      <th>AAAS</th>\n",
       "      <th>AACS</th>\n",
       "      <th>AAGAB</th>\n",
       "      <th>...</th>\n",
       "      <th>ZUP1</th>\n",
       "      <th>ZW10</th>\n",
       "      <th>ZWILCH</th>\n",
       "      <th>ZWINT</th>\n",
       "      <th>ZXDA</th>\n",
       "      <th>ZXDB</th>\n",
       "      <th>ZXDC</th>\n",
       "      <th>ZYG11B</th>\n",
       "      <th>ZYX</th>\n",
       "      <th>ZZEF1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.157851</td>\n",
       "      <td>0.135139</td>\n",
       "      <td>0.064579</td>\n",
       "      <td>0.188614</td>\n",
       "      <td>0.246985</td>\n",
       "      <td>0.129225</td>\n",
       "      <td>-0.205016</td>\n",
       "      <td>0.159521</td>\n",
       "      <td>-0.132249</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.120751</td>\n",
       "      <td>-0.119468</td>\n",
       "      <td>-0.055896</td>\n",
       "      <td>-0.013762</td>\n",
       "      <td>0.252637</td>\n",
       "      <td>0.194140</td>\n",
       "      <td>0.214614</td>\n",
       "      <td>0.097543</td>\n",
       "      <td>-0.469122</td>\n",
       "      <td>-0.143820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.082970</td>\n",
       "      <td>0.112493</td>\n",
       "      <td>0.016122</td>\n",
       "      <td>0.150402</td>\n",
       "      <td>0.242833</td>\n",
       "      <td>0.162413</td>\n",
       "      <td>-0.177303</td>\n",
       "      <td>0.130744</td>\n",
       "      <td>-0.135339</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.110065</td>\n",
       "      <td>-0.138271</td>\n",
       "      <td>-0.091429</td>\n",
       "      <td>-0.037659</td>\n",
       "      <td>0.190040</td>\n",
       "      <td>0.114548</td>\n",
       "      <td>0.159004</td>\n",
       "      <td>0.075738</td>\n",
       "      <td>-0.352956</td>\n",
       "      <td>-0.133433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.086733</td>\n",
       "      <td>0.106412</td>\n",
       "      <td>0.018158</td>\n",
       "      <td>0.150171</td>\n",
       "      <td>0.226735</td>\n",
       "      <td>0.199774</td>\n",
       "      <td>-0.161684</td>\n",
       "      <td>0.133260</td>\n",
       "      <td>-0.131192</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.103275</td>\n",
       "      <td>-0.134534</td>\n",
       "      <td>-0.091236</td>\n",
       "      <td>-0.031357</td>\n",
       "      <td>0.194296</td>\n",
       "      <td>0.116869</td>\n",
       "      <td>0.160172</td>\n",
       "      <td>0.077250</td>\n",
       "      <td>-0.335472</td>\n",
       "      <td>-0.131992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.051763</td>\n",
       "      <td>0.067409</td>\n",
       "      <td>-0.025391</td>\n",
       "      <td>0.013669</td>\n",
       "      <td>0.008115</td>\n",
       "      <td>0.049289</td>\n",
       "      <td>-0.155272</td>\n",
       "      <td>0.105142</td>\n",
       "      <td>-0.134851</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.089427</td>\n",
       "      <td>-0.113620</td>\n",
       "      <td>-0.090622</td>\n",
       "      <td>-0.036129</td>\n",
       "      <td>0.144817</td>\n",
       "      <td>0.089914</td>\n",
       "      <td>0.145952</td>\n",
       "      <td>0.108163</td>\n",
       "      <td>-0.242637</td>\n",
       "      <td>-0.141276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.054975</td>\n",
       "      <td>0.057908</td>\n",
       "      <td>-0.013780</td>\n",
       "      <td>0.038065</td>\n",
       "      <td>0.045544</td>\n",
       "      <td>0.077910</td>\n",
       "      <td>-0.145470</td>\n",
       "      <td>0.100929</td>\n",
       "      <td>-0.135718</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.094463</td>\n",
       "      <td>-0.127061</td>\n",
       "      <td>-0.106618</td>\n",
       "      <td>-0.045987</td>\n",
       "      <td>0.147557</td>\n",
       "      <td>0.083280</td>\n",
       "      <td>0.135690</td>\n",
       "      <td>0.073483</td>\n",
       "      <td>-0.270784</td>\n",
       "      <td>-0.155530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>250</td>\n",
       "      <td>-0.009482</td>\n",
       "      <td>0.043044</td>\n",
       "      <td>-0.058341</td>\n",
       "      <td>-0.033009</td>\n",
       "      <td>-0.093903</td>\n",
       "      <td>-0.090123</td>\n",
       "      <td>-0.132484</td>\n",
       "      <td>0.072513</td>\n",
       "      <td>-0.140553</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.074358</td>\n",
       "      <td>-0.113390</td>\n",
       "      <td>-0.108952</td>\n",
       "      <td>-0.058431</td>\n",
       "      <td>0.086136</td>\n",
       "      <td>0.043518</td>\n",
       "      <td>0.118689</td>\n",
       "      <td>0.097737</td>\n",
       "      <td>-0.131085</td>\n",
       "      <td>-0.139192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>251</td>\n",
       "      <td>0.182772</td>\n",
       "      <td>0.117246</td>\n",
       "      <td>0.048263</td>\n",
       "      <td>0.107544</td>\n",
       "      <td>0.247084</td>\n",
       "      <td>0.418682</td>\n",
       "      <td>-0.143986</td>\n",
       "      <td>0.158732</td>\n",
       "      <td>-0.110476</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.087298</td>\n",
       "      <td>-0.094408</td>\n",
       "      <td>-0.057521</td>\n",
       "      <td>0.017400</td>\n",
       "      <td>0.249831</td>\n",
       "      <td>0.187176</td>\n",
       "      <td>0.209753</td>\n",
       "      <td>0.115751</td>\n",
       "      <td>-0.366013</td>\n",
       "      <td>-0.140920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>252</td>\n",
       "      <td>-0.043379</td>\n",
       "      <td>-0.004612</td>\n",
       "      <td>-0.069868</td>\n",
       "      <td>-0.039644</td>\n",
       "      <td>-0.113420</td>\n",
       "      <td>-0.117409</td>\n",
       "      <td>-0.109536</td>\n",
       "      <td>0.040984</td>\n",
       "      <td>-0.147273</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069852</td>\n",
       "      <td>-0.132072</td>\n",
       "      <td>-0.144296</td>\n",
       "      <td>-0.085587</td>\n",
       "      <td>0.052804</td>\n",
       "      <td>0.003479</td>\n",
       "      <td>0.075373</td>\n",
       "      <td>0.049226</td>\n",
       "      <td>-0.096919</td>\n",
       "      <td>-0.155979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>253</td>\n",
       "      <td>0.443533</td>\n",
       "      <td>0.528896</td>\n",
       "      <td>0.447688</td>\n",
       "      <td>0.966563</td>\n",
       "      <td>0.989150</td>\n",
       "      <td>0.925471</td>\n",
       "      <td>-0.064117</td>\n",
       "      <td>0.476835</td>\n",
       "      <td>-0.064779</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.058895</td>\n",
       "      <td>0.321025</td>\n",
       "      <td>0.264615</td>\n",
       "      <td>0.296434</td>\n",
       "      <td>0.621609</td>\n",
       "      <td>0.593143</td>\n",
       "      <td>0.443181</td>\n",
       "      <td>0.285706</td>\n",
       "      <td>-0.514714</td>\n",
       "      <td>0.033446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>254</td>\n",
       "      <td>0.000453</td>\n",
       "      <td>0.052817</td>\n",
       "      <td>-0.031914</td>\n",
       "      <td>0.062507</td>\n",
       "      <td>0.040163</td>\n",
       "      <td>0.041464</td>\n",
       "      <td>-0.127123</td>\n",
       "      <td>0.092584</td>\n",
       "      <td>-0.142807</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.079904</td>\n",
       "      <td>-0.139591</td>\n",
       "      <td>-0.129375</td>\n",
       "      <td>-0.057819</td>\n",
       "      <td>0.120919</td>\n",
       "      <td>0.034853</td>\n",
       "      <td>0.083771</td>\n",
       "      <td>0.048612</td>\n",
       "      <td>-0.145312</td>\n",
       "      <td>-0.133178</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>255 rows  18212 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id      A1BG  A1BG-AS1       A2M   A2M-AS1     A2MP1    A4GALT  \\\n",
       "0      0  0.157851  0.135139  0.064579  0.188614  0.246985  0.129225   \n",
       "1      1  0.082970  0.112493  0.016122  0.150402  0.242833  0.162413   \n",
       "2      2  0.086733  0.106412  0.018158  0.150171  0.226735  0.199774   \n",
       "3      3  0.051763  0.067409 -0.025391  0.013669  0.008115  0.049289   \n",
       "4      4  0.054975  0.057908 -0.013780  0.038065  0.045544  0.077910   \n",
       "..   ...       ...       ...       ...       ...       ...       ...   \n",
       "250  250 -0.009482  0.043044 -0.058341 -0.033009 -0.093903 -0.090123   \n",
       "251  251  0.182772  0.117246  0.048263  0.107544  0.247084  0.418682   \n",
       "252  252 -0.043379 -0.004612 -0.069868 -0.039644 -0.113420 -0.117409   \n",
       "253  253  0.443533  0.528896  0.447688  0.966563  0.989150  0.925471   \n",
       "254  254  0.000453  0.052817 -0.031914  0.062507  0.040163  0.041464   \n",
       "\n",
       "         AAAS      AACS     AAGAB  ...      ZUP1      ZW10    ZWILCH  \\\n",
       "0   -0.205016  0.159521 -0.132249  ... -0.120751 -0.119468 -0.055896   \n",
       "1   -0.177303  0.130744 -0.135339  ... -0.110065 -0.138271 -0.091429   \n",
       "2   -0.161684  0.133260 -0.131192  ... -0.103275 -0.134534 -0.091236   \n",
       "3   -0.155272  0.105142 -0.134851  ... -0.089427 -0.113620 -0.090622   \n",
       "4   -0.145470  0.100929 -0.135718  ... -0.094463 -0.127061 -0.106618   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "250 -0.132484  0.072513 -0.140553  ... -0.074358 -0.113390 -0.108952   \n",
       "251 -0.143986  0.158732 -0.110476  ... -0.087298 -0.094408 -0.057521   \n",
       "252 -0.109536  0.040984 -0.147273  ... -0.069852 -0.132072 -0.144296   \n",
       "253 -0.064117  0.476835 -0.064779  ... -0.058895  0.321025  0.264615   \n",
       "254 -0.127123  0.092584 -0.142807  ... -0.079904 -0.139591 -0.129375   \n",
       "\n",
       "        ZWINT      ZXDA      ZXDB      ZXDC    ZYG11B       ZYX     ZZEF1  \n",
       "0   -0.013762  0.252637  0.194140  0.214614  0.097543 -0.469122 -0.143820  \n",
       "1   -0.037659  0.190040  0.114548  0.159004  0.075738 -0.352956 -0.133433  \n",
       "2   -0.031357  0.194296  0.116869  0.160172  0.077250 -0.335472 -0.131992  \n",
       "3   -0.036129  0.144817  0.089914  0.145952  0.108163 -0.242637 -0.141276  \n",
       "4   -0.045987  0.147557  0.083280  0.135690  0.073483 -0.270784 -0.155530  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "250 -0.058431  0.086136  0.043518  0.118689  0.097737 -0.131085 -0.139192  \n",
       "251  0.017400  0.249831  0.187176  0.209753  0.115751 -0.366013 -0.140920  \n",
       "252 -0.085587  0.052804  0.003479  0.075373  0.049226 -0.096919 -0.155979  \n",
       "253  0.296434  0.621609  0.593143  0.443181  0.285706 -0.514714  0.033446  \n",
       "254 -0.057819  0.120919  0.034853  0.083771  0.048612 -0.145312 -0.133178  \n",
       "\n",
       "[255 rows x 18212 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_ = pd.read_csv('data/submission_5.csv')\n",
    "pred_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1BG</th>\n",
       "      <th>A1BG-AS1</th>\n",
       "      <th>A2M</th>\n",
       "      <th>A2M-AS1</th>\n",
       "      <th>A2MP1</th>\n",
       "      <th>A4GALT</th>\n",
       "      <th>AAAS</th>\n",
       "      <th>AACS</th>\n",
       "      <th>AAGAB</th>\n",
       "      <th>AAK1</th>\n",
       "      <th>...</th>\n",
       "      <th>ZUP1</th>\n",
       "      <th>ZW10</th>\n",
       "      <th>ZWILCH</th>\n",
       "      <th>ZWINT</th>\n",
       "      <th>ZXDA</th>\n",
       "      <th>ZXDB</th>\n",
       "      <th>ZXDC</th>\n",
       "      <th>ZYG11B</th>\n",
       "      <th>ZYX</th>\n",
       "      <th>ZZEF1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.218221</td>\n",
       "      <td>0.179731</td>\n",
       "      <td>-0.054798</td>\n",
       "      <td>0.024871</td>\n",
       "      <td>0.493190</td>\n",
       "      <td>0.581067</td>\n",
       "      <td>-0.055398</td>\n",
       "      <td>0.232303</td>\n",
       "      <td>-0.069473</td>\n",
       "      <td>0.001176</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009535</td>\n",
       "      <td>0.052298</td>\n",
       "      <td>-0.105074</td>\n",
       "      <td>0.135251</td>\n",
       "      <td>0.341514</td>\n",
       "      <td>0.257881</td>\n",
       "      <td>0.153122</td>\n",
       "      <td>0.099098</td>\n",
       "      <td>-0.224445</td>\n",
       "      <td>-0.131643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.146980</td>\n",
       "      <td>0.038844</td>\n",
       "      <td>-0.144263</td>\n",
       "      <td>-0.234001</td>\n",
       "      <td>0.115186</td>\n",
       "      <td>0.338515</td>\n",
       "      <td>-0.070514</td>\n",
       "      <td>0.154567</td>\n",
       "      <td>-0.093607</td>\n",
       "      <td>-0.010897</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.112410</td>\n",
       "      <td>-0.008791</td>\n",
       "      <td>-0.160951</td>\n",
       "      <td>0.059692</td>\n",
       "      <td>0.214382</td>\n",
       "      <td>0.146892</td>\n",
       "      <td>0.106424</td>\n",
       "      <td>0.080884</td>\n",
       "      <td>-0.237877</td>\n",
       "      <td>-0.148247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.260201</td>\n",
       "      <td>0.128910</td>\n",
       "      <td>0.008374</td>\n",
       "      <td>-0.068510</td>\n",
       "      <td>0.356836</td>\n",
       "      <td>0.640791</td>\n",
       "      <td>-0.032256</td>\n",
       "      <td>0.253137</td>\n",
       "      <td>-0.039208</td>\n",
       "      <td>0.028447</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034453</td>\n",
       "      <td>0.077636</td>\n",
       "      <td>-0.067954</td>\n",
       "      <td>0.163916</td>\n",
       "      <td>0.350582</td>\n",
       "      <td>0.258399</td>\n",
       "      <td>0.176241</td>\n",
       "      <td>0.123509</td>\n",
       "      <td>-0.196821</td>\n",
       "      <td>-0.107288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.252309</td>\n",
       "      <td>0.121778</td>\n",
       "      <td>-0.002467</td>\n",
       "      <td>-0.081616</td>\n",
       "      <td>0.337699</td>\n",
       "      <td>0.623014</td>\n",
       "      <td>-0.033744</td>\n",
       "      <td>0.247436</td>\n",
       "      <td>-0.040560</td>\n",
       "      <td>0.030770</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.040627</td>\n",
       "      <td>0.072673</td>\n",
       "      <td>-0.075436</td>\n",
       "      <td>0.157450</td>\n",
       "      <td>0.341365</td>\n",
       "      <td>0.250930</td>\n",
       "      <td>0.171957</td>\n",
       "      <td>0.122709</td>\n",
       "      <td>-0.197713</td>\n",
       "      <td>-0.109039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.179937</td>\n",
       "      <td>0.068156</td>\n",
       "      <td>-0.097399</td>\n",
       "      <td>-0.180143</td>\n",
       "      <td>0.193829</td>\n",
       "      <td>0.445006</td>\n",
       "      <td>-0.053160</td>\n",
       "      <td>0.188950</td>\n",
       "      <td>-0.071118</td>\n",
       "      <td>0.021154</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.087039</td>\n",
       "      <td>0.019429</td>\n",
       "      <td>-0.134014</td>\n",
       "      <td>0.092073</td>\n",
       "      <td>0.258866</td>\n",
       "      <td>0.186053</td>\n",
       "      <td>0.130042</td>\n",
       "      <td>0.099978</td>\n",
       "      <td>-0.215229</td>\n",
       "      <td>-0.130735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>0.262735</td>\n",
       "      <td>0.137643</td>\n",
       "      <td>0.010280</td>\n",
       "      <td>-0.052465</td>\n",
       "      <td>0.380265</td>\n",
       "      <td>0.651925</td>\n",
       "      <td>-0.032190</td>\n",
       "      <td>0.257872</td>\n",
       "      <td>-0.038919</td>\n",
       "      <td>0.032956</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026895</td>\n",
       "      <td>0.081294</td>\n",
       "      <td>-0.067910</td>\n",
       "      <td>0.166742</td>\n",
       "      <td>0.360593</td>\n",
       "      <td>0.266977</td>\n",
       "      <td>0.178082</td>\n",
       "      <td>0.123737</td>\n",
       "      <td>-0.195710</td>\n",
       "      <td>-0.107242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>0.229755</td>\n",
       "      <td>0.119831</td>\n",
       "      <td>-0.035955</td>\n",
       "      <td>-0.085194</td>\n",
       "      <td>0.332474</td>\n",
       "      <td>0.576855</td>\n",
       "      <td>-0.044853</td>\n",
       "      <td>0.230195</td>\n",
       "      <td>-0.055966</td>\n",
       "      <td>0.015955</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.042312</td>\n",
       "      <td>0.057204</td>\n",
       "      <td>-0.094581</td>\n",
       "      <td>0.140514</td>\n",
       "      <td>0.324180</td>\n",
       "      <td>0.237511</td>\n",
       "      <td>0.158389</td>\n",
       "      <td>0.110620</td>\n",
       "      <td>-0.211132</td>\n",
       "      <td>-0.120791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>0.219246</td>\n",
       "      <td>0.111294</td>\n",
       "      <td>-0.050836</td>\n",
       "      <td>-0.100878</td>\n",
       "      <td>0.309572</td>\n",
       "      <td>0.547538</td>\n",
       "      <td>-0.047613</td>\n",
       "      <td>0.222076</td>\n",
       "      <td>-0.060479</td>\n",
       "      <td>0.012676</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.049701</td>\n",
       "      <td>0.049368</td>\n",
       "      <td>-0.101029</td>\n",
       "      <td>0.127463</td>\n",
       "      <td>0.312100</td>\n",
       "      <td>0.227682</td>\n",
       "      <td>0.153151</td>\n",
       "      <td>0.106363</td>\n",
       "      <td>-0.215378</td>\n",
       "      <td>-0.124312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>0.266777</td>\n",
       "      <td>0.198003</td>\n",
       "      <td>0.006471</td>\n",
       "      <td>0.058443</td>\n",
       "      <td>0.542213</td>\n",
       "      <td>0.705397</td>\n",
       "      <td>-0.041422</td>\n",
       "      <td>0.273030</td>\n",
       "      <td>-0.048167</td>\n",
       "      <td>0.010054</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025350</td>\n",
       "      <td>0.090661</td>\n",
       "      <td>-0.062945</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>0.396374</td>\n",
       "      <td>0.299574</td>\n",
       "      <td>0.183930</td>\n",
       "      <td>0.114505</td>\n",
       "      <td>-0.214198</td>\n",
       "      <td>-0.116457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>0.202584</td>\n",
       "      <td>0.092384</td>\n",
       "      <td>-0.070552</td>\n",
       "      <td>-0.135625</td>\n",
       "      <td>0.258835</td>\n",
       "      <td>0.504495</td>\n",
       "      <td>-0.053548</td>\n",
       "      <td>0.207348</td>\n",
       "      <td>-0.069096</td>\n",
       "      <td>0.008265</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.066069</td>\n",
       "      <td>0.036302</td>\n",
       "      <td>-0.116292</td>\n",
       "      <td>0.113463</td>\n",
       "      <td>0.290806</td>\n",
       "      <td>0.209000</td>\n",
       "      <td>0.142768</td>\n",
       "      <td>0.100492</td>\n",
       "      <td>-0.219275</td>\n",
       "      <td>-0.129686</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>255 rows  18211 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         A1BG  A1BG-AS1       A2M   A2M-AS1     A2MP1    A4GALT      AAAS  \\\n",
       "0    0.218221  0.179731 -0.054798  0.024871  0.493190  0.581067 -0.055398   \n",
       "1    0.146980  0.038844 -0.144263 -0.234001  0.115186  0.338515 -0.070514   \n",
       "2    0.260201  0.128910  0.008374 -0.068510  0.356836  0.640791 -0.032256   \n",
       "3    0.252309  0.121778 -0.002467 -0.081616  0.337699  0.623014 -0.033744   \n",
       "4    0.179937  0.068156 -0.097399 -0.180143  0.193829  0.445006 -0.053160   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "250  0.262735  0.137643  0.010280 -0.052465  0.380265  0.651925 -0.032190   \n",
       "251  0.229755  0.119831 -0.035955 -0.085194  0.332474  0.576855 -0.044853   \n",
       "252  0.219246  0.111294 -0.050836 -0.100878  0.309572  0.547538 -0.047613   \n",
       "253  0.266777  0.198003  0.006471  0.058443  0.542213  0.705397 -0.041422   \n",
       "254  0.202584  0.092384 -0.070552 -0.135625  0.258835  0.504495 -0.053548   \n",
       "\n",
       "         AACS     AAGAB      AAK1  ...      ZUP1      ZW10    ZWILCH  \\\n",
       "0    0.232303 -0.069473  0.001176  ...  0.009535  0.052298 -0.105074   \n",
       "1    0.154567 -0.093607 -0.010897  ... -0.112410 -0.008791 -0.160951   \n",
       "2    0.253137 -0.039208  0.028447  ... -0.034453  0.077636 -0.067954   \n",
       "3    0.247436 -0.040560  0.030770  ... -0.040627  0.072673 -0.075436   \n",
       "4    0.188950 -0.071118  0.021154  ... -0.087039  0.019429 -0.134014   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "250  0.257872 -0.038919  0.032956  ... -0.026895  0.081294 -0.067910   \n",
       "251  0.230195 -0.055966  0.015955  ... -0.042312  0.057204 -0.094581   \n",
       "252  0.222076 -0.060479  0.012676  ... -0.049701  0.049368 -0.101029   \n",
       "253  0.273030 -0.048167  0.010054  ...  0.025350  0.090661 -0.062945   \n",
       "254  0.207348 -0.069096  0.008265  ... -0.066069  0.036302 -0.116292   \n",
       "\n",
       "        ZWINT      ZXDA      ZXDB      ZXDC    ZYG11B       ZYX     ZZEF1  \n",
       "0    0.135251  0.341514  0.257881  0.153122  0.099098 -0.224445 -0.131643  \n",
       "1    0.059692  0.214382  0.146892  0.106424  0.080884 -0.237877 -0.148247  \n",
       "2    0.163916  0.350582  0.258399  0.176241  0.123509 -0.196821 -0.107288  \n",
       "3    0.157450  0.341365  0.250930  0.171957  0.122709 -0.197713 -0.109039  \n",
       "4    0.092073  0.258866  0.186053  0.130042  0.099978 -0.215229 -0.130735  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "250  0.166742  0.360593  0.266977  0.178082  0.123737 -0.195710 -0.107242  \n",
       "251  0.140514  0.324180  0.237511  0.158389  0.110620 -0.211132 -0.120791  \n",
       "252  0.127463  0.312100  0.227682  0.153151  0.106363 -0.215378 -0.124312  \n",
       "253  0.179200  0.396374  0.299574  0.183930  0.114505 -0.214198 -0.116457  \n",
       "254  0.113463  0.290806  0.209000  0.142768  0.100492 -0.219275 -0.129686  \n",
       "\n",
       "[255 rows x 18211 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
