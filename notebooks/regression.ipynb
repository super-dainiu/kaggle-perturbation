{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('/home/yz979/code/kaggle-perturbation/')\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '5'\n",
    "\n",
    "train_path = 'data/de_train.parquet'\n",
    "submit_path = 'data/id_map.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import anndata\n",
    "import networkx as nx\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Any, Union, Optional\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv, GATConv, SAGEConv\n",
    "from torch_geometric.nn.norm import BatchNorm, GraphNorm\n",
    "from torch.utils.data import Dataset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularGraphDataset(Dataset):\n",
    "    def __init__(self, df, cond_cols, target_cols, perturb_key, edge_index=None, edge_attr=None, scaler=None):\n",
    "        self.df = df\n",
    "        self.cond_cols = cond_cols\n",
    "        self.perturb_key = perturb_key\n",
    "        self.target_cols = target_cols\n",
    "        self.cat_dict = {}\n",
    "        self.edge_index = edge_index\n",
    "        self.edge_attr = edge_attr\n",
    "        self.scaler = scaler\n",
    "\n",
    "        for cond_col in cond_cols:\n",
    "            unique_values = df[cond_col].unique()\n",
    "            self.cat_dict[cond_col] = {val: i for i, val in enumerate(unique_values)}\n",
    "        \n",
    "        unique_values = df[perturb_key].unique()\n",
    "        self.cat_dict[perturb_key] = {val: i for i, val in enumerate(unique_values)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        ret_dict = {}\n",
    "        for cond_col in self.cond_cols:\n",
    "            ret_dict[cond_col] = torch.tensor(self.cat_dict[cond_col][row[cond_col]])\n",
    "        ret_dict[self.perturb_key] = torch.tensor(self.cat_dict[self.perturb_key][row[self.perturb_key]])\n",
    "        target = torch.tensor(row[self.target_cols].values.astype(np.float32)) if self.target_cols else torch.tensor([0])\n",
    "        return Data(x=None, edge_index=self.edge_index, edge_attr=self.edge_attr, **ret_dict), target\n",
    "\n",
    "    def set_graph(self, edge_index, edge_attr):\n",
    "        self.edge_index = edge_index\n",
    "        self.edge_attr = edge_attr\n",
    "\n",
    "    def encode(self, val, col):\n",
    "        return self.cat_dict[col][val]\n",
    "\n",
    "    def decode(self, val, col):\n",
    "        return list(self.cat_dict[col].keys())[val]\n",
    "\n",
    "    def vocab_size(self, col):\n",
    "        return len(self.cat_dict[col])\n",
    "\n",
    "def validate_config(dataset, config):\n",
    "    config['perturb_key'] = dataset.perturb_key\n",
    "    for cond_col in dataset.cond_cols:\n",
    "        config[f'num_{cond_col}s'] = dataset.vocab_size(cond_col)\n",
    "    config[f'num_{dataset.perturb_key}s'] = dataset.vocab_size(dataset.perturb_key)\n",
    "    config['num_targets'] = len(dataset.target_cols)\n",
    "    config['conditions'] = dataset.cond_cols\n",
    "    return config\n",
    "\n",
    "def load_dataset(\n",
    "    path,\n",
    "    cond_cols = ['cell_type'],\n",
    "    perturb_key = 'perturb',\n",
    "    col_map = {'cell_type': 'cell_type', 'sm_lincs_id': 'perturb'},\n",
    "    is_test = False,\n",
    "    scaler = None,\n",
    "    cat_dict = None,\n",
    "):\n",
    "    df = pd.read_parquet(path)\n",
    "    df = df.rename(columns=col_map)\n",
    "    target_cols = df.iloc[:, 5:].columns.tolist() if not is_test else None\n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "        df.iloc[:, 5:] = scaler.fit_transform(df.iloc[:, 5:])\n",
    "    ds = TabularGraphDataset(df, cond_cols, target_cols, perturb_key, scaler=scaler)\n",
    "    if cat_dict:\n",
    "        ds.cat_dict = cat_dict\n",
    "    return ds\n",
    "\n",
    "@dataclass\n",
    "class GeneSimNetwork():\n",
    "    G: nx.DiGraph\n",
    "    edge_index: torch.Tensor\n",
    "    edge_weight: torch.Tensor\n",
    "    \n",
    "    @classmethod\n",
    "    def from_edges(\n",
    "        cls,\n",
    "        edge_list: pd.DataFrame,\n",
    "        gene_list: List,\n",
    "        node_map: Dict[str, int],\n",
    "        ) -> \"GeneSimNetwork\":\n",
    "        \"\"\"\n",
    "        Generate gene similarity network from edge list\n",
    "\n",
    "        Args:\n",
    "            edge_list (pd.DataFrame): edge list of the network\n",
    "            gene_list (list): list of gene names\n",
    "            node_map (dict): dictionary mapping gene names to node indices\n",
    "\n",
    "        Returns:\n",
    "            GeneSimNetwork: gene similarity network\n",
    "        \"\"\"\n",
    "        G = nx.from_pandas_edgelist(edge_list, source='source',\n",
    "                                    target='target', edge_attr=['importance'],\n",
    "                                    create_using=nx.DiGraph())\n",
    "        for n in gene_list:\n",
    "            if n not in G.nodes():\n",
    "                G.add_node(n)\n",
    "\n",
    "        to_remove = []\n",
    "        for n in G.nodes():\n",
    "            if n not in gene_list:\n",
    "                to_remove.append(n)\n",
    "        \n",
    "        for n in to_remove:\n",
    "            G.remove_node(n)\n",
    "\n",
    "        edge_index_ = [(node_map[e[0]], node_map[e[1]]) for e in G.edges]\n",
    "\n",
    "        edge_index = torch.tensor(edge_index_, dtype=torch.long).T\n",
    "        \n",
    "        edge_attr = nx.get_edge_attributes(G, 'importance') \n",
    "        importance = np.array([edge_attr[e] for e in G.edges])\n",
    "        edge_weight = torch.Tensor(importance)\n",
    "        \n",
    "        return cls(G, edge_index, edge_weight)\n",
    "\n",
    "def load_gene_network(path, gene_list, node_map, threshold=0.5):\n",
    "    edge_list = pd.read_csv(path)\n",
    "    print('Number of edges before filtering: ', len(edge_list))\n",
    "    edge_list = edge_list[edge_list['importance'] > threshold]\n",
    "    print('Number of edges after filtering: ', len(edge_list))\n",
    "    return GeneSimNetwork.from_edges(edge_list, gene_list, node_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mrrmse_loss(predicted, actual):\n",
    "    squared_error = (predicted - actual) ** 2\n",
    "    rowwise_mse = torch.mean(squared_error, dim=1)\n",
    "    rowwise_rmse = torch.sqrt(rowwise_mse)\n",
    "    mrrmse = torch.mean(rowwise_rmse)\n",
    "    return mrrmse\n",
    "\n",
    "def train_epoch(model, optimizer, dataloader, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for i, (data, target) in enumerate(dataloader):\n",
    "        data.to(device)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(**data.to_dict())\n",
    "        loss = mrrmse_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def validate_epoch(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, target) in enumerate(dataloader):\n",
    "            data.to(device)\n",
    "            target = target.to(device)\n",
    "            output = model(**data.to_dict())\n",
    "            loss = mrrmse_loss(output, target)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionEncoder(nn.Module):\n",
    "    def __init__(self, config, condition):\n",
    "        super(ConditionEncoder, self).__init__()\n",
    "        self.config = config\n",
    "        self.condition = condition\n",
    "        self.embeddings = nn.Embedding(self.config[f'num_{self.condition}s'], self.config['embed_size'])\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(self.config['embed_size'], self.config['hidden_size']),\n",
    "            nn.BatchNorm1d(self.config['hidden_size']) if self.config.get('batch_norm') else nn.Identity(),\n",
    "            nn.Dropout(self.config['dropout']),\n",
    "        )\n",
    "\n",
    "    def set_embedding(self, embedding, freeze=True):\n",
    "        self.embeddings = nn.Embedding.from_pretrained(embedding, freeze=freeze)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(self.embeddings(x))\n",
    "\n",
    "class PerturbationEncoder(nn.Module):\n",
    "    def __init__(self, config, condition):\n",
    "        super(PerturbationEncoder, self).__init__()\n",
    "        raise NotImplementedError\n",
    "\n",
    "class CellEncoder(nn.Module):\n",
    "    perturb_encoder_cls = ConditionEncoder  # PerturbationEncoder is not implemented yet (TODO)\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(CellEncoder, self).__init__()\n",
    "        self.config = config\n",
    "        self.cell_embedding = nn.Parameter(torch.empty(1, self.config['num_targets'], self.config['embed_size']))  # replace with better embedding\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(self.config['embed_size'], self.config['hidden_size']),\n",
    "            nn.LayerNorm(self.config['hidden_size']) if self.config.get('layer_norm') else nn.Identity(),\n",
    "            nn.Dropout(self.config['dropout']),\n",
    "        )\n",
    "        for condition in self.config['conditions']:\n",
    "            setattr(self, f'{condition}_encoder', ConditionEncoder(self.config, condition))\n",
    "        self.perturb_encoder = self.perturb_encoder_cls(self.config, config['perturb_key'])\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.cell_embedding)\n",
    "\n",
    "    def set_cell_embedding(self, cell_embedding, freeze=True):\n",
    "        self.cell_embedding = nn.Parameter(cell_embedding, requires_grad=not freeze)\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        cell_embedding = self.cell_embedding\n",
    "        condition_embedding = torch.cat([\n",
    "            getattr(self, f'{condition}_encoder')(kwargs[condition]).unsqueeze(1)\n",
    "            for condition in self.config['conditions']\n",
    "            ], dim=1)\n",
    "        condition_embedding = condition_embedding.sum(dim=1, keepdim=True)\n",
    "        perturb_embedding = self.perturb_encoder(kwargs[self.config['perturb_key']]).unsqueeze(1)\n",
    "        return self.encoder(cell_embedding), condition_embedding, perturb_embedding\n",
    "\n",
    "class ValueDecoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(ValueDecoder, self).__init__()\n",
    "        self.config = config\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.config['hidden_size'], self.config['hidden_size'] * 2),\n",
    "            BatchNorm(self.config['num_targets']) if self.config.get('batch_norm') else nn.Identity(),\n",
    "            nn.Dropout(self.config['dropout']),\n",
    "            config['activation'],\n",
    "            nn.Linear(self.config['hidden_size'] * 2, self.config['hidden_size']),\n",
    "            BatchNorm(self.config['num_targets']) if self.config.get('batch_norm') else nn.Identity(),\n",
    "            nn.Dropout(self.config['dropout']),\n",
    "            config['activation'],\n",
    "            nn.Linear(self.config['hidden_size'], 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(x).squeeze(-1)\n",
    "\n",
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.config = config\n",
    "        self.graph_conv_1 = GCNConv(self.config['hidden_size'], self.config['hidden_size'])\n",
    "        self.graph_conv_2 = GCNConv(self.config['hidden_size'], self.config['hidden_size'])\n",
    "        self.dropout = nn.Dropout(self.config['dropout'])\n",
    "        self.act_fn = config['activation']\n",
    "        \n",
    "        self.batch_norm_1 = BatchNorm(self.config['num_targets']) if self.config.get('batch_norm') else nn.Identity()\n",
    "        self.batch_norm_2 = BatchNorm(self.config['num_targets']) if self.config.get('batch_norm') else nn.Identity()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None):\n",
    "        res = x\n",
    "        x = self.batch_norm_1(self.dropout(self.act_fn(self.graph_conv_1(x, edge_index, edge_attr))))\n",
    "        x = self.batch_norm_2(self.dropout(self.act_fn(self.graph_conv_2(x, edge_index, edge_attr))))\n",
    "        return x + res\n",
    "\n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(TransformerLayer, self).__init__()\n",
    "        self.config = config\n",
    "        self.attn = nn.MultiheadAttention(self.config['hidden_size'], self.config['num_heads'])\n",
    "        self.layer_norm_1 = nn.LayerNorm(self.config['hidden_size'])\n",
    "        \n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(self.config['hidden_size'], self.config['hidden_size'] * 4),\n",
    "            nn.Dropout(self.config['dropout']),\n",
    "            nn.GELU(), \n",
    "            nn.Linear(self.config['hidden_size'] * 4, self.config['hidden_size']),\n",
    "        )\n",
    "        self.layer_norm_2 = nn.LayerNorm(self.config['hidden_size'])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.attn(x, x, x)[0] + x\n",
    "        x = self.layer_norm_1(x)\n",
    "        x = self.ffn(x) + x\n",
    "        x = self.layer_norm_2(x)\n",
    "        return x\n",
    "\n",
    "class DERegressor(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(DERegressor, self).__init__()\n",
    "        self.config = config\n",
    "        self.cell_encoder = CellEncoder(self.config)\n",
    "        self.value_decoder = ValueDecoder(self.config)\n",
    "        \n",
    "        self.w_x = nn.Parameter(torch.empty(self.config['num_targets'], 1))\n",
    "        self.w_c = nn.Parameter(torch.empty(1, self.config['hidden_size']))\n",
    "        self.w_p = nn.Parameter(torch.empty(1, self.config['hidden_size']))\n",
    "\n",
    "        self.edge_index = nn.Parameter(None, requires_grad=False)\n",
    "        self.edge_attr = nn.Parameter(None, requires_grad=False)\n",
    "        if self.config['model_type'] == 'gcn':\n",
    "            self.gcn_layers = nn.ModuleList([GCNLayer(self.config) for _ in range(self.config['num_layers'])])\n",
    "        elif self.config['model_type'] == 'transformer':\n",
    "            self.transformer_layers = nn.ModuleList([TransformerLayer(self.config) for _ in range(self.config['num_layers'])])\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.w_x)\n",
    "        nn.init.xavier_uniform_(self.w_c)\n",
    "        nn.init.xavier_uniform_(self.w_p)\n",
    "    \n",
    "    def forward(self, **kwargs):\n",
    "        x, c, p = self.cell_encoder(**kwargs)\n",
    "        x = x * self.w_x + c * self.w_c + p * self.w_p\n",
    "        if self.config['model_type'] == 'gcn':\n",
    "            for gcn_layer in self.gcn_layers:\n",
    "                x = gcn_layer(x, self.edge_index, self.edge_attr)\n",
    "        elif self.config['model_type'] == 'transformer':\n",
    "            for transformer_layer in self.transformer_layers:\n",
    "                x = transformer_layer(x)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return self.value_decoder(x)\n",
    "\n",
    "    def set_cell_embedding(self, cell_embedding, freeze=True):\n",
    "        self.cell_encoder.set_cell_embedding(cell_embedding, freeze=freeze)\n",
    "\n",
    "    def set_condition_embedding(self, condition, condition_embedding, freeze=True):\n",
    "        getattr(self.cell_encoder, f'{condition}_encoder').set_embedding(condition_embedding, freeze=freeze)\n",
    "    \n",
    "    def set_graph(self, edge_index, edge_attr):\n",
    "        self.edge_index = nn.Parameter(edge_index, requires_grad=False)\n",
    "        self.edge_attr = nn.Parameter(edge_attr, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges before filtering:  15801928\n",
      "Number of edges after filtering:  613872\n",
      "Number of nodes:  18211\n",
      "Number of edges:  613872\n",
      "Number of connected components:  8809\n",
      "Number of isolated nodes:  5531\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(train_path)\n",
    "submission = load_dataset(submit_path, is_test=True, cat_dict=dataset.cat_dict, scaler=dataset.scaler)\n",
    "\n",
    "gene_list = dataset.df.columns[5:].tolist()\n",
    "node_map = {gene: i for i, gene in enumerate(gene_list)}\n",
    "gene_sim_network = load_gene_network('data/grn/go.csv', gene_list, node_map, threshold=0.3)\n",
    "\n",
    "# statistics of gene similarity network\n",
    "print('Number of nodes: ', gene_sim_network.G.number_of_nodes())\n",
    "print('Number of edges: ', gene_sim_network.G.number_of_edges())\n",
    "print('Number of connected components: ', nx.number_weakly_connected_components(gene_sim_network.G))\n",
    "print('Number of isolated nodes: ', len(list(nx.isolates(gene_sim_network.G))))\n",
    "# degree distribution\n",
    "degrees = [d for n, d in gene_sim_network.G.degree()]\n",
    "plt.hist(degrees, bins=100)\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    'hidden_size': 256,\n",
    "    'embed_size': 128,\n",
    "    'activation': nn.LeakyReLU(),\n",
    "    'num_layers': 2,\n",
    "    'num_heads': 8,\n",
    "    'batch_norm': True,\n",
    "    'layer_norm': False,\n",
    "    'dropout': 0.2,\n",
    "    'model_type': 'gcn',\n",
    "}\n",
    "model_config = validate_config(dataset, model_config)\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [int(len(dataset) * 0.9), len(dataset) - int(len(dataset) * 0.9)], generator=torch.Generator().manual_seed(42))\n",
    "train_loader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "geneformer_embeddings = torch.load('models/geneformer_embeddings_.pt')\n",
    "chemberta_embeddings = torch.load('models/chemberta_embeddings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss: 0.582655388039428 val rmse: 0.6380996964871883\n",
      "tensor(4.5917, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7043, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.8193, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Saving model with val loss 0.6380996964871883\n",
      "Epoch 1 train loss: 0.5877190205183896 val rmse: 0.5614994913339615\n",
      "tensor(4.6300, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7500, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.8033, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Saving model with val loss 0.5614994913339615\n",
      "Epoch 2 train loss: 0.5812996602677679 val rmse: 0.5966787822544575\n",
      "tensor(4.7135, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7292, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.8166, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 3 train loss: 0.5820647408435871 val rmse: 0.588374026119709\n",
      "tensor(4.6509, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7241, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.8253, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 4 train loss: 0.5797995305680609 val rmse: 0.6003181263804436\n",
      "tensor(4.6247, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7127, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.8218, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 5 train loss: 0.5866887023696652 val rmse: 0.6364708691835403\n",
      "tensor(4.6947, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7002, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.8165, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 6 train loss: 0.5827734214918954 val rmse: 0.636796098202467\n",
      "tensor(4.7364, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7101, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.8232, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 7 train loss: 0.5878561262960558 val rmse: 0.6878826804459095\n",
      "tensor(4.5887, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7055, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.8207, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 8 train loss: 0.5752587155862288 val rmse: 0.6244123913347721\n",
      "tensor(4.6785, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7075, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.8209, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 9 train loss: 0.5749941088936545 val rmse: 0.5945015139877796\n",
      "tensor(4.7139, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7135, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.8084, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 10 train loss: 0.5737163846368913 val rmse: 0.5632516071200371\n",
      "tensor(4.7028, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7236, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.8001, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 11 train loss: 0.5702258340724102 val rmse: 0.6439008377492428\n",
      "tensor(4.7454, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7111, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.8060, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 12 train loss: 0.5649612530485376 val rmse: 0.5758532546460629\n",
      "tensor(4.7245, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7049, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.8115, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 13 train loss: 0.5649233758449554 val rmse: 0.5448502413928509\n",
      "tensor(4.7629, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7067, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.8074, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Saving model with val loss 0.5448502413928509\n",
      "Epoch 14 train loss: 0.5616367935360252 val rmse: 0.5547479912638664\n",
      "tensor(4.7139, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7027, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.8088, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 15 train loss: 0.5618674317737679 val rmse: 0.5795771107077599\n",
      "tensor(4.7460, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7069, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.8046, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 16 train loss: 0.5617852210998535 val rmse: 0.5622722208499908\n",
      "tensor(4.7422, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7025, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.8070, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 17 train loss: 0.5607969582854927 val rmse: 0.5728605724871159\n",
      "tensor(4.7032, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.6984, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.8092, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 18 train loss: 0.5600922510221407 val rmse: 0.5647861994802952\n",
      "tensor(4.7097, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.6978, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.8078, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 19 train loss: 0.5596612498357698 val rmse: 0.5987362191081047\n",
      "tensor(4.7025, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7014, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.8070, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 20 train loss: 0.5579277002192163 val rmse: 0.5685864314436913\n",
      "tensor(4.6991, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7037, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.8043, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 21 train loss: 0.5569148485536699 val rmse: 0.5793242454528809\n",
      "tensor(4.7074, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7000, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.8058, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 22 train loss: 0.5591410321074647 val rmse: 0.5861433893442154\n",
      "tensor(4.6977, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7025, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.8015, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 23 train loss: 0.5577798857317342 val rmse: 0.5608580969274044\n",
      "tensor(4.6885, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7043, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7989, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 24 train loss: 0.5566077414271119 val rmse: 0.5514436140656471\n",
      "tensor(4.6878, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7067, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7982, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 25 train loss: 0.5606554824036437 val rmse: 0.5771796517074108\n",
      "tensor(4.7034, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7061, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7965, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 26 train loss: 0.5602764158279865 val rmse: 0.5349544063210487\n",
      "tensor(4.6780, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.6989, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7961, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Saving model with val loss 0.5349544063210487\n",
      "Epoch 27 train loss: 0.5608521963094736 val rmse: 0.5571829676628113\n",
      "tensor(4.6833, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7041, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7867, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 28 train loss: 0.5610714682510921 val rmse: 0.5667903088033199\n",
      "tensor(4.6460, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7094, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7855, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 29 train loss: 0.5615154438978666 val rmse: 0.5703850090503693\n",
      "tensor(4.6628, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7044, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7820, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 30 train loss: 0.5704625895271054 val rmse: 0.5590776316821575\n",
      "tensor(4.6904, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7457, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7531, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 31 train loss: 0.5658106722614982 val rmse: 0.6557983495295048\n",
      "tensor(4.6753, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7189, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7598, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 32 train loss: 0.5721232391797103 val rmse: 0.5766468197107315\n",
      "tensor(4.7470, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.6863, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7896, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 33 train loss: 0.5679279410993898 val rmse: 0.5939499735832214\n",
      "tensor(4.7360, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.6917, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7775, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 34 train loss: 0.5793055168219975 val rmse: 0.5909902043640614\n",
      "tensor(4.7522, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7206, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7724, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 35 train loss: 0.5868897898630663 val rmse: 0.6049053817987442\n",
      "tensor(4.7240, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7873, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7206, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 36 train loss: 0.5953101008743434 val rmse: 0.7575718685984612\n",
      "tensor(4.7705, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7798, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7516, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 37 train loss: 0.6029818851452369 val rmse: 0.6253011599183083\n",
      "tensor(4.8215, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7734, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7605, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 38 train loss: 0.5859257640776696 val rmse: 0.5608036145567894\n",
      "tensor(4.7527, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7515, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7801, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 39 train loss: 0.5752643749311372 val rmse: 0.642745852470398\n",
      "tensor(4.8386, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7523, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7745, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 40 train loss: 0.5801040383128376 val rmse: 0.5719320848584175\n",
      "tensor(4.8355, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7728, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7639, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 41 train loss: 0.5836898022657865 val rmse: 0.5562217682600021\n",
      "tensor(4.8392, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7688, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7852, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 42 train loss: 0.5964282960086674 val rmse: 0.5504171252250671\n",
      "tensor(4.8160, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8346, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7566, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 43 train loss: 0.581921927340619 val rmse: 0.6516831293702126\n",
      "tensor(4.7752, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7963, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7666, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 44 train loss: 0.5758092333744098 val rmse: 0.5417759120464325\n",
      "tensor(4.8501, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8216, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7522, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 45 train loss: 0.5696023799382247 val rmse: 0.5712491720914841\n",
      "tensor(4.9569, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8196, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7440, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 46 train loss: 0.5697797258178909 val rmse: 0.5680140070617199\n",
      "tensor(4.9343, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7993, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7661, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 47 train loss: 0.5635344951183765 val rmse: 0.5624428503215313\n",
      "tensor(4.8681, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8214, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7477, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 48 train loss: 0.564686682317164 val rmse: 0.549656767398119\n",
      "tensor(4.8866, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8059, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7557, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 49 train loss: 0.5608913097288701 val rmse: 0.5489163100719452\n",
      "tensor(4.8460, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8091, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7599, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 50 train loss: 0.5577378199472056 val rmse: 0.6082062982022762\n",
      "tensor(4.8563, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8018, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7626, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 51 train loss: 0.5594649427122884 val rmse: 0.5546995289623737\n",
      "tensor(4.9347, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7809, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7739, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 52 train loss: 0.5590668715439834 val rmse: 0.5376826822757721\n",
      "tensor(4.9103, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7810, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7722, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 53 train loss: 0.5579123980813212 val rmse: 0.5637666583061218\n",
      "tensor(4.8944, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7793, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7691, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 54 train loss: 0.5560598381153949 val rmse: 0.5642764493823051\n",
      "tensor(4.8612, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7748, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7729, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 55 train loss: 0.5516137623941744 val rmse: 0.5639739744365215\n",
      "tensor(4.8619, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7740, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7704, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 56 train loss: 0.5602324380503072 val rmse: 0.5560482740402222\n",
      "tensor(4.8771, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7867, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7629, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 57 train loss: 0.5537215088094983 val rmse: 0.5652009844779968\n",
      "tensor(4.8762, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7851, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7604, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 58 train loss: 0.552229975338106 val rmse: 0.5530516095459461\n",
      "tensor(4.8718, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7824, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7599, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 59 train loss: 0.5516492457358868 val rmse: 0.5506282635033131\n",
      "tensor(4.8689, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7835, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7590, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 60 train loss: 0.5504245936096489 val rmse: 0.5604498982429504\n",
      "tensor(4.8675, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7833, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7575, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 61 train loss: 0.5508483872785197 val rmse: 0.5432901680469513\n",
      "tensor(4.8647, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7850, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7550, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 62 train loss: 0.5505089086371583 val rmse: 0.5538915731012821\n",
      "tensor(4.8584, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7837, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7553, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 63 train loss: 0.5519721229355057 val rmse: 0.5648499056696892\n",
      "tensor(4.8512, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7780, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7570, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 64 train loss: 0.5502260002222928 val rmse: 0.5488244853913784\n",
      "tensor(4.8691, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7804, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7524, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 65 train loss: 0.5499879663640802 val rmse: 0.5423340015113354\n",
      "tensor(4.8753, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7838, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7479, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 66 train loss: 0.5515247095714916 val rmse: 0.5658925697207451\n",
      "tensor(4.8628, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7860, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7448, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 67 train loss: 0.5514400636220907 val rmse: 0.5758293941617012\n",
      "tensor(4.8575, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7818, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7491, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 68 train loss: 0.5533643848710246 val rmse: 0.6096517965197563\n",
      "tensor(4.8461, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7736, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7480, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 69 train loss: 0.5523114057330342 val rmse: 0.578429538756609\n",
      "tensor(4.8673, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7713, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7469, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 70 train loss: 0.5516640958073852 val rmse: 0.5364166647195816\n",
      "tensor(4.8892, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7697, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7400, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 71 train loss: 0.5546649597681962 val rmse: 0.5410264022648335\n",
      "tensor(4.8519, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7806, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7324, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 72 train loss: 0.5593046743374366 val rmse: 0.6257349029183388\n",
      "tensor(4.9008, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7654, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7397, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 73 train loss: 0.5597126952239445 val rmse: 0.5871938429772854\n",
      "tensor(4.9045, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7565, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7410, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 74 train loss: 0.5605894245884635 val rmse: 0.549824945628643\n",
      "tensor(4.9618, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7790, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7388, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 75 train loss: 0.5619788862668075 val rmse: 0.5729885809123516\n",
      "tensor(5.0115, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8050, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7046, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 76 train loss: 0.5697224774917999 val rmse: 0.5572209879755974\n",
      "tensor(5.0266, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7810, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7034, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 77 train loss: 0.5617471979810046 val rmse: 0.5715109966695309\n",
      "tensor(5.0340, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7724, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7137, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 78 train loss: 0.563248257745396 val rmse: 0.5673008859157562\n",
      "tensor(5.0075, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7520, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7175, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 79 train loss: 0.5679269820064693 val rmse: 0.6199661865830421\n",
      "tensor(5.0291, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7628, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6945, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 80 train loss: 0.5774517705688229 val rmse: 0.5518125370144844\n",
      "tensor(5.1698, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.8004, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6601, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 81 train loss: 0.5749385519461199 val rmse: 0.5600337199866772\n",
      "tensor(5.0877, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7888, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6964, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 82 train loss: 0.5682863387968633 val rmse: 0.5510201714932919\n",
      "tensor(5.1389, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7280, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7322, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 83 train loss: 0.5675618478230068 val rmse: 0.5694955512881279\n",
      "tensor(5.2121, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7497, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7168, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 84 train loss: 0.5617945875440326 val rmse: 0.5725370720028877\n",
      "tensor(5.1759, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7346, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7236, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 85 train loss: 0.5582604675323932 val rmse: 0.5396096184849739\n",
      "tensor(5.2053, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7494, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7091, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 86 train loss: 0.5536777648058805 val rmse: 0.5487034842371941\n",
      "tensor(5.2210, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7603, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7013, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 87 train loss: 0.5542635263560655 val rmse: 0.5672893524169922\n",
      "tensor(5.2361, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7402, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7069, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 88 train loss: 0.5516704320907593 val rmse: 0.5623261295258999\n",
      "tensor(5.2879, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7378, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.7026, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 89 train loss: 0.5523452325300737 val rmse: 0.5425737425684929\n",
      "tensor(5.2773, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7570, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6885, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 90 train loss: 0.5573306021752296 val rmse: 0.5489429607987404\n",
      "tensor(5.2743, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7657, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6805, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 91 train loss: 0.5525396698481077 val rmse: 0.5324516706168652\n",
      "tensor(5.2628, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7512, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6825, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Saving model with val loss 0.5324516706168652\n",
      "Epoch 92 train loss: 0.550441492300529 val rmse: 0.5702586956322193\n",
      "tensor(5.2895, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7459, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6759, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 93 train loss: 0.5468147900197413 val rmse: 0.5576508231461048\n",
      "tensor(5.3030, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7457, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6721, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 94 train loss: 0.5520285522782957 val rmse: 0.5530159808695316\n",
      "tensor(5.2710, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7405, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6742, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 95 train loss: 0.5481011844300604 val rmse: 0.5722132809460163\n",
      "tensor(5.2649, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7386, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6751, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 96 train loss: 0.5466110884369194 val rmse: 0.5492562428116798\n",
      "tensor(5.2730, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7368, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6736, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 97 train loss: 0.5446989783993015 val rmse: 0.5605880208313465\n",
      "tensor(5.2865, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7401, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6686, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 98 train loss: 0.5444602757305294 val rmse: 0.5573581084609032\n",
      "tensor(5.2737, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7391, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6665, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Epoch 99 train loss: 0.5431728842970612 val rmse: 0.5817626751959324\n",
      "tensor(5.2713, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(1.7374, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>) tensor(2.6664, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "experiment_name = 'grn_model_3'\n",
    "\n",
    "# model = DERegressor(model_config)\n",
    "# model.w_p.data *= 2\n",
    "# model.w_x.data *= 5\n",
    "model.set_cell_embedding(geneformer_embeddings, freeze=True)\n",
    "model.set_condition_embedding('perturb', chemberta_embeddings, freeze=True)\n",
    "model.set_graph(gene_sim_network.edge_index, gene_sim_network.edge_weight)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20, eta_min=1e-4)\n",
    "\n",
    "model = model.to(device)\n",
    "best_val_loss = np.inf\n",
    "for epoch in range(100):\n",
    "    train_loss = train_epoch(model, optimizer, train_loader, device)\n",
    "    val_loss = validate_epoch(model, val_loader, device)\n",
    "    print(f'Epoch {epoch} train loss: {train_loss} val rmse: {val_loss}')\n",
    "    scheduler.step()\n",
    "    print(torch.norm(model.w_x), torch.norm(model.w_c), torch.norm(model.w_p))\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        print(f'Saving model with val loss {val_loss}')\n",
    "        torch.save(model.state_dict(), f'models/{experiment_name}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'models/{experiment_name}.pt')\n",
    "model = DERegressor(model_config)\n",
    "model.set_graph(gene_sim_network.edge_index, gene_sim_network.edge_weight)\n",
    "model.load_state_dict(torch.load('models/grn_model_3.pt'))\n",
    "model = model.to(device)\n",
    "test_loader = DataLoader(submission, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1BG</th>\n",
       "      <th>A1BG-AS1</th>\n",
       "      <th>A2M</th>\n",
       "      <th>A2M-AS1</th>\n",
       "      <th>A2MP1</th>\n",
       "      <th>A4GALT</th>\n",
       "      <th>AAAS</th>\n",
       "      <th>AACS</th>\n",
       "      <th>AAGAB</th>\n",
       "      <th>AAK1</th>\n",
       "      <th>...</th>\n",
       "      <th>ZUP1</th>\n",
       "      <th>ZW10</th>\n",
       "      <th>ZWILCH</th>\n",
       "      <th>ZWINT</th>\n",
       "      <th>ZXDA</th>\n",
       "      <th>ZXDB</th>\n",
       "      <th>ZXDC</th>\n",
       "      <th>ZYG11B</th>\n",
       "      <th>ZYX</th>\n",
       "      <th>ZZEF1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.146269</td>\n",
       "      <td>0.183023</td>\n",
       "      <td>0.332441</td>\n",
       "      <td>0.414301</td>\n",
       "      <td>0.672281</td>\n",
       "      <td>0.552283</td>\n",
       "      <td>-0.060240</td>\n",
       "      <td>0.301165</td>\n",
       "      <td>-0.063933</td>\n",
       "      <td>0.086928</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.049229</td>\n",
       "      <td>0.065833</td>\n",
       "      <td>-0.012736</td>\n",
       "      <td>0.131011</td>\n",
       "      <td>0.413950</td>\n",
       "      <td>0.221799</td>\n",
       "      <td>0.263083</td>\n",
       "      <td>0.247864</td>\n",
       "      <td>-0.270937</td>\n",
       "      <td>-0.002731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.045753</td>\n",
       "      <td>0.091397</td>\n",
       "      <td>0.031774</td>\n",
       "      <td>0.109496</td>\n",
       "      <td>0.191179</td>\n",
       "      <td>0.234520</td>\n",
       "      <td>-0.080836</td>\n",
       "      <td>0.153269</td>\n",
       "      <td>-0.087724</td>\n",
       "      <td>0.087088</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.050081</td>\n",
       "      <td>-0.024475</td>\n",
       "      <td>-0.079846</td>\n",
       "      <td>0.055823</td>\n",
       "      <td>0.221112</td>\n",
       "      <td>0.127865</td>\n",
       "      <td>0.167732</td>\n",
       "      <td>0.124091</td>\n",
       "      <td>-0.178990</td>\n",
       "      <td>-0.073314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.213900</td>\n",
       "      <td>0.114297</td>\n",
       "      <td>-0.014126</td>\n",
       "      <td>0.085559</td>\n",
       "      <td>0.588788</td>\n",
       "      <td>0.960837</td>\n",
       "      <td>-0.073110</td>\n",
       "      <td>0.124631</td>\n",
       "      <td>-0.089561</td>\n",
       "      <td>0.087334</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035062</td>\n",
       "      <td>-0.044204</td>\n",
       "      <td>-0.073328</td>\n",
       "      <td>0.133334</td>\n",
       "      <td>0.312190</td>\n",
       "      <td>0.185615</td>\n",
       "      <td>0.154190</td>\n",
       "      <td>0.070604</td>\n",
       "      <td>-0.153526</td>\n",
       "      <td>-0.124553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.033717</td>\n",
       "      <td>0.093775</td>\n",
       "      <td>0.022903</td>\n",
       "      <td>0.068832</td>\n",
       "      <td>0.216591</td>\n",
       "      <td>0.148574</td>\n",
       "      <td>-0.084055</td>\n",
       "      <td>0.156686</td>\n",
       "      <td>-0.041752</td>\n",
       "      <td>0.087111</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031935</td>\n",
       "      <td>-0.030856</td>\n",
       "      <td>-0.104787</td>\n",
       "      <td>-0.005999</td>\n",
       "      <td>0.166350</td>\n",
       "      <td>0.132469</td>\n",
       "      <td>0.169434</td>\n",
       "      <td>0.182007</td>\n",
       "      <td>-0.230734</td>\n",
       "      <td>-0.034873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.147944</td>\n",
       "      <td>0.168414</td>\n",
       "      <td>0.024061</td>\n",
       "      <td>0.035586</td>\n",
       "      <td>0.469285</td>\n",
       "      <td>0.446196</td>\n",
       "      <td>-0.050396</td>\n",
       "      <td>0.219444</td>\n",
       "      <td>-0.017575</td>\n",
       "      <td>0.087426</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022863</td>\n",
       "      <td>0.039758</td>\n",
       "      <td>-0.061425</td>\n",
       "      <td>0.113763</td>\n",
       "      <td>0.261257</td>\n",
       "      <td>0.199587</td>\n",
       "      <td>0.168443</td>\n",
       "      <td>0.201024</td>\n",
       "      <td>-0.209801</td>\n",
       "      <td>-0.049670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>0.053958</td>\n",
       "      <td>-0.113918</td>\n",
       "      <td>-0.046797</td>\n",
       "      <td>0.070163</td>\n",
       "      <td>1.226450</td>\n",
       "      <td>-0.026877</td>\n",
       "      <td>-0.080774</td>\n",
       "      <td>-0.009201</td>\n",
       "      <td>-0.172793</td>\n",
       "      <td>0.088712</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.117756</td>\n",
       "      <td>-0.183555</td>\n",
       "      <td>-0.215538</td>\n",
       "      <td>-0.128316</td>\n",
       "      <td>0.234174</td>\n",
       "      <td>0.043567</td>\n",
       "      <td>0.065664</td>\n",
       "      <td>0.037525</td>\n",
       "      <td>-0.142958</td>\n",
       "      <td>-0.127991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>-0.016329</td>\n",
       "      <td>-0.151038</td>\n",
       "      <td>-0.178194</td>\n",
       "      <td>0.013775</td>\n",
       "      <td>1.984082</td>\n",
       "      <td>-0.246001</td>\n",
       "      <td>-0.108613</td>\n",
       "      <td>-0.082603</td>\n",
       "      <td>-0.076495</td>\n",
       "      <td>0.088168</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.051949</td>\n",
       "      <td>-0.156913</td>\n",
       "      <td>-0.268021</td>\n",
       "      <td>-0.191560</td>\n",
       "      <td>0.150827</td>\n",
       "      <td>0.027597</td>\n",
       "      <td>0.005115</td>\n",
       "      <td>-0.055681</td>\n",
       "      <td>-0.092981</td>\n",
       "      <td>-0.190351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>0.088451</td>\n",
       "      <td>-0.104633</td>\n",
       "      <td>-0.292230</td>\n",
       "      <td>-0.144710</td>\n",
       "      <td>2.637277</td>\n",
       "      <td>0.151753</td>\n",
       "      <td>-0.108687</td>\n",
       "      <td>-0.071291</td>\n",
       "      <td>-0.119444</td>\n",
       "      <td>0.088254</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.097905</td>\n",
       "      <td>-0.214567</td>\n",
       "      <td>-0.259252</td>\n",
       "      <td>-0.116624</td>\n",
       "      <td>0.242646</td>\n",
       "      <td>0.015748</td>\n",
       "      <td>-0.045661</td>\n",
       "      <td>-0.141710</td>\n",
       "      <td>-0.110951</td>\n",
       "      <td>-0.204862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>2.827626</td>\n",
       "      <td>1.024067</td>\n",
       "      <td>-4.059803</td>\n",
       "      <td>2.349225</td>\n",
       "      <td>17.326069</td>\n",
       "      <td>4.039629</td>\n",
       "      <td>0.623714</td>\n",
       "      <td>0.180134</td>\n",
       "      <td>2.420457</td>\n",
       "      <td>0.087822</td>\n",
       "      <td>...</td>\n",
       "      <td>0.073284</td>\n",
       "      <td>0.612915</td>\n",
       "      <td>-3.970440</td>\n",
       "      <td>-0.511649</td>\n",
       "      <td>1.468711</td>\n",
       "      <td>-1.123937</td>\n",
       "      <td>-1.454470</td>\n",
       "      <td>0.800975</td>\n",
       "      <td>-0.220075</td>\n",
       "      <td>-0.028759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>0.131576</td>\n",
       "      <td>-0.009661</td>\n",
       "      <td>-0.227935</td>\n",
       "      <td>0.130773</td>\n",
       "      <td>2.823965</td>\n",
       "      <td>0.238293</td>\n",
       "      <td>-0.060695</td>\n",
       "      <td>0.005285</td>\n",
       "      <td>0.021051</td>\n",
       "      <td>0.088013</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057371</td>\n",
       "      <td>-0.090017</td>\n",
       "      <td>-0.220824</td>\n",
       "      <td>-0.006320</td>\n",
       "      <td>0.288810</td>\n",
       "      <td>0.092882</td>\n",
       "      <td>0.014669</td>\n",
       "      <td>-0.001056</td>\n",
       "      <td>-0.152877</td>\n",
       "      <td>-0.182069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>255 rows × 18211 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         A1BG  A1BG-AS1       A2M   A2M-AS1      A2MP1    A4GALT      AAAS  \\\n",
       "0    0.146269  0.183023  0.332441  0.414301   0.672281  0.552283 -0.060240   \n",
       "1    0.045753  0.091397  0.031774  0.109496   0.191179  0.234520 -0.080836   \n",
       "2    0.213900  0.114297 -0.014126  0.085559   0.588788  0.960837 -0.073110   \n",
       "3    0.033717  0.093775  0.022903  0.068832   0.216591  0.148574 -0.084055   \n",
       "4    0.147944  0.168414  0.024061  0.035586   0.469285  0.446196 -0.050396   \n",
       "..        ...       ...       ...       ...        ...       ...       ...   \n",
       "250  0.053958 -0.113918 -0.046797  0.070163   1.226450 -0.026877 -0.080774   \n",
       "251 -0.016329 -0.151038 -0.178194  0.013775   1.984082 -0.246001 -0.108613   \n",
       "252  0.088451 -0.104633 -0.292230 -0.144710   2.637277  0.151753 -0.108687   \n",
       "253  2.827626  1.024067 -4.059803  2.349225  17.326069  4.039629  0.623714   \n",
       "254  0.131576 -0.009661 -0.227935  0.130773   2.823965  0.238293 -0.060695   \n",
       "\n",
       "         AACS     AAGAB      AAK1  ...      ZUP1      ZW10    ZWILCH  \\\n",
       "0    0.301165 -0.063933  0.086928  ... -0.049229  0.065833 -0.012736   \n",
       "1    0.153269 -0.087724  0.087088  ... -0.050081 -0.024475 -0.079846   \n",
       "2    0.124631 -0.089561  0.087334  ... -0.035062 -0.044204 -0.073328   \n",
       "3    0.156686 -0.041752  0.087111  ... -0.031935 -0.030856 -0.104787   \n",
       "4    0.219444 -0.017575  0.087426  ... -0.022863  0.039758 -0.061425   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "250 -0.009201 -0.172793  0.088712  ... -0.117756 -0.183555 -0.215538   \n",
       "251 -0.082603 -0.076495  0.088168  ... -0.051949 -0.156913 -0.268021   \n",
       "252 -0.071291 -0.119444  0.088254  ... -0.097905 -0.214567 -0.259252   \n",
       "253  0.180134  2.420457  0.087822  ...  0.073284  0.612915 -3.970440   \n",
       "254  0.005285  0.021051  0.088013  ... -0.057371 -0.090017 -0.220824   \n",
       "\n",
       "        ZWINT      ZXDA      ZXDB      ZXDC    ZYG11B       ZYX     ZZEF1  \n",
       "0    0.131011  0.413950  0.221799  0.263083  0.247864 -0.270937 -0.002731  \n",
       "1    0.055823  0.221112  0.127865  0.167732  0.124091 -0.178990 -0.073314  \n",
       "2    0.133334  0.312190  0.185615  0.154190  0.070604 -0.153526 -0.124553  \n",
       "3   -0.005999  0.166350  0.132469  0.169434  0.182007 -0.230734 -0.034873  \n",
       "4    0.113763  0.261257  0.199587  0.168443  0.201024 -0.209801 -0.049670  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "250 -0.128316  0.234174  0.043567  0.065664  0.037525 -0.142958 -0.127991  \n",
       "251 -0.191560  0.150827  0.027597  0.005115 -0.055681 -0.092981 -0.190351  \n",
       "252 -0.116624  0.242646  0.015748 -0.045661 -0.141710 -0.110951 -0.204862  \n",
       "253 -0.511649  1.468711 -1.123937 -1.454470  0.800975 -0.220075 -0.028759  \n",
       "254 -0.006320  0.288810  0.092882  0.014669 -0.001056 -0.152877 -0.182069  \n",
       "\n",
       "[255 rows x 18211 columns]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# submit\n",
    "model.eval()\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for i, (data, _) in enumerate(test_loader):\n",
    "        data.to(device)\n",
    "        output = model(**data.to_dict())\n",
    "        preds.append(output.cpu().numpy())\n",
    "\n",
    "preds = np.concatenate(preds)\n",
    "preds = dataset.scaler.inverse_transform(preds)\n",
    "preds = pd.DataFrame(preds, columns=dataset.target_cols)\n",
    "preds.to_csv('data/submission_6.csv', index_label='id')\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cell_type</th>\n",
       "      <th>sm_name</th>\n",
       "      <th>perturb</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>B cells</td>\n",
       "      <td>Vorinostat</td>\n",
       "      <td>LSM-3828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>Myeloid cells</td>\n",
       "      <td>Vorinostat</td>\n",
       "      <td>LSM-3828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         cell_type     sm_name   perturb\n",
       "id                                      \n",
       "126        B cells  Vorinostat  LSM-3828\n",
       "253  Myeloid cells  Vorinostat  LSM-3828"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# submission.df.perturb == submission.decode(15, 'perturb')\n",
    "find = submission.df[submission.df.perturb == submission.decode(15, 'perturb')]\n",
    "find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1BG</th>\n",
       "      <th>A1BG-AS1</th>\n",
       "      <th>A2M</th>\n",
       "      <th>A2M-AS1</th>\n",
       "      <th>A2MP1</th>\n",
       "      <th>A4GALT</th>\n",
       "      <th>AAAS</th>\n",
       "      <th>AACS</th>\n",
       "      <th>AAGAB</th>\n",
       "      <th>AAK1</th>\n",
       "      <th>...</th>\n",
       "      <th>ZUP1</th>\n",
       "      <th>ZW10</th>\n",
       "      <th>ZWILCH</th>\n",
       "      <th>ZWINT</th>\n",
       "      <th>ZXDA</th>\n",
       "      <th>ZXDB</th>\n",
       "      <th>ZXDC</th>\n",
       "      <th>ZYG11B</th>\n",
       "      <th>ZYX</th>\n",
       "      <th>ZZEF1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.187844</td>\n",
       "      <td>0.343249</td>\n",
       "      <td>-1.367830</td>\n",
       "      <td>-0.624914</td>\n",
       "      <td>2.002171</td>\n",
       "      <td>0.418658</td>\n",
       "      <td>0.221087</td>\n",
       "      <td>0.227081</td>\n",
       "      <td>2.394212</td>\n",
       "      <td>0.087044</td>\n",
       "      <td>...</td>\n",
       "      <td>0.129910</td>\n",
       "      <td>0.221945</td>\n",
       "      <td>-1.98317</td>\n",
       "      <td>-0.009505</td>\n",
       "      <td>0.102313</td>\n",
       "      <td>0.055104</td>\n",
       "      <td>-0.459282</td>\n",
       "      <td>0.630109</td>\n",
       "      <td>-0.268355</td>\n",
       "      <td>-0.082340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>2.827626</td>\n",
       "      <td>1.024067</td>\n",
       "      <td>-4.059803</td>\n",
       "      <td>2.349225</td>\n",
       "      <td>17.326069</td>\n",
       "      <td>4.039629</td>\n",
       "      <td>0.623714</td>\n",
       "      <td>0.180134</td>\n",
       "      <td>2.420457</td>\n",
       "      <td>0.087822</td>\n",
       "      <td>...</td>\n",
       "      <td>0.073284</td>\n",
       "      <td>0.612915</td>\n",
       "      <td>-3.97044</td>\n",
       "      <td>-0.511649</td>\n",
       "      <td>1.468711</td>\n",
       "      <td>-1.123937</td>\n",
       "      <td>-1.454470</td>\n",
       "      <td>0.800975</td>\n",
       "      <td>-0.220075</td>\n",
       "      <td>-0.028759</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 18211 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         A1BG  A1BG-AS1       A2M   A2M-AS1      A2MP1    A4GALT      AAAS  \\\n",
       "126  0.187844  0.343249 -1.367830 -0.624914   2.002171  0.418658  0.221087   \n",
       "253  2.827626  1.024067 -4.059803  2.349225  17.326069  4.039629  0.623714   \n",
       "\n",
       "         AACS     AAGAB      AAK1  ...      ZUP1      ZW10   ZWILCH     ZWINT  \\\n",
       "126  0.227081  2.394212  0.087044  ...  0.129910  0.221945 -1.98317 -0.009505   \n",
       "253  0.180134  2.420457  0.087822  ...  0.073284  0.612915 -3.97044 -0.511649   \n",
       "\n",
       "         ZXDA      ZXDB      ZXDC    ZYG11B       ZYX     ZZEF1  \n",
       "126  0.102313  0.055104 -0.459282  0.630109 -0.268355 -0.082340  \n",
       "253  1.468711 -1.123937 -1.454470  0.800975 -0.220075 -0.028759  \n",
       "\n",
       "[2 rows x 18211 columns]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.iloc[find.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>A1BG</th>\n",
       "      <th>A1BG-AS1</th>\n",
       "      <th>A2M</th>\n",
       "      <th>A2M-AS1</th>\n",
       "      <th>A2MP1</th>\n",
       "      <th>A4GALT</th>\n",
       "      <th>AAAS</th>\n",
       "      <th>AACS</th>\n",
       "      <th>AAGAB</th>\n",
       "      <th>...</th>\n",
       "      <th>ZUP1</th>\n",
       "      <th>ZW10</th>\n",
       "      <th>ZWILCH</th>\n",
       "      <th>ZWINT</th>\n",
       "      <th>ZXDA</th>\n",
       "      <th>ZXDB</th>\n",
       "      <th>ZXDC</th>\n",
       "      <th>ZYG11B</th>\n",
       "      <th>ZYX</th>\n",
       "      <th>ZZEF1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.157851</td>\n",
       "      <td>0.135139</td>\n",
       "      <td>0.064579</td>\n",
       "      <td>0.188614</td>\n",
       "      <td>0.246985</td>\n",
       "      <td>0.129225</td>\n",
       "      <td>-0.205016</td>\n",
       "      <td>0.159521</td>\n",
       "      <td>-0.132249</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.120751</td>\n",
       "      <td>-0.119468</td>\n",
       "      <td>-0.055896</td>\n",
       "      <td>-0.013762</td>\n",
       "      <td>0.252637</td>\n",
       "      <td>0.194140</td>\n",
       "      <td>0.214614</td>\n",
       "      <td>0.097543</td>\n",
       "      <td>-0.469122</td>\n",
       "      <td>-0.143820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.082970</td>\n",
       "      <td>0.112493</td>\n",
       "      <td>0.016122</td>\n",
       "      <td>0.150402</td>\n",
       "      <td>0.242833</td>\n",
       "      <td>0.162413</td>\n",
       "      <td>-0.177303</td>\n",
       "      <td>0.130744</td>\n",
       "      <td>-0.135339</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.110065</td>\n",
       "      <td>-0.138271</td>\n",
       "      <td>-0.091429</td>\n",
       "      <td>-0.037659</td>\n",
       "      <td>0.190040</td>\n",
       "      <td>0.114548</td>\n",
       "      <td>0.159004</td>\n",
       "      <td>0.075738</td>\n",
       "      <td>-0.352956</td>\n",
       "      <td>-0.133433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.086733</td>\n",
       "      <td>0.106412</td>\n",
       "      <td>0.018158</td>\n",
       "      <td>0.150171</td>\n",
       "      <td>0.226735</td>\n",
       "      <td>0.199774</td>\n",
       "      <td>-0.161684</td>\n",
       "      <td>0.133260</td>\n",
       "      <td>-0.131192</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.103275</td>\n",
       "      <td>-0.134534</td>\n",
       "      <td>-0.091236</td>\n",
       "      <td>-0.031357</td>\n",
       "      <td>0.194296</td>\n",
       "      <td>0.116869</td>\n",
       "      <td>0.160172</td>\n",
       "      <td>0.077250</td>\n",
       "      <td>-0.335472</td>\n",
       "      <td>-0.131992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.051763</td>\n",
       "      <td>0.067409</td>\n",
       "      <td>-0.025391</td>\n",
       "      <td>0.013669</td>\n",
       "      <td>0.008115</td>\n",
       "      <td>0.049289</td>\n",
       "      <td>-0.155272</td>\n",
       "      <td>0.105142</td>\n",
       "      <td>-0.134851</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.089427</td>\n",
       "      <td>-0.113620</td>\n",
       "      <td>-0.090622</td>\n",
       "      <td>-0.036129</td>\n",
       "      <td>0.144817</td>\n",
       "      <td>0.089914</td>\n",
       "      <td>0.145952</td>\n",
       "      <td>0.108163</td>\n",
       "      <td>-0.242637</td>\n",
       "      <td>-0.141276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.054975</td>\n",
       "      <td>0.057908</td>\n",
       "      <td>-0.013780</td>\n",
       "      <td>0.038065</td>\n",
       "      <td>0.045544</td>\n",
       "      <td>0.077910</td>\n",
       "      <td>-0.145470</td>\n",
       "      <td>0.100929</td>\n",
       "      <td>-0.135718</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.094463</td>\n",
       "      <td>-0.127061</td>\n",
       "      <td>-0.106618</td>\n",
       "      <td>-0.045987</td>\n",
       "      <td>0.147557</td>\n",
       "      <td>0.083280</td>\n",
       "      <td>0.135690</td>\n",
       "      <td>0.073483</td>\n",
       "      <td>-0.270784</td>\n",
       "      <td>-0.155530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>250</td>\n",
       "      <td>-0.009482</td>\n",
       "      <td>0.043044</td>\n",
       "      <td>-0.058341</td>\n",
       "      <td>-0.033009</td>\n",
       "      <td>-0.093903</td>\n",
       "      <td>-0.090123</td>\n",
       "      <td>-0.132484</td>\n",
       "      <td>0.072513</td>\n",
       "      <td>-0.140553</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.074358</td>\n",
       "      <td>-0.113390</td>\n",
       "      <td>-0.108952</td>\n",
       "      <td>-0.058431</td>\n",
       "      <td>0.086136</td>\n",
       "      <td>0.043518</td>\n",
       "      <td>0.118689</td>\n",
       "      <td>0.097737</td>\n",
       "      <td>-0.131085</td>\n",
       "      <td>-0.139192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>251</td>\n",
       "      <td>0.182772</td>\n",
       "      <td>0.117246</td>\n",
       "      <td>0.048263</td>\n",
       "      <td>0.107544</td>\n",
       "      <td>0.247084</td>\n",
       "      <td>0.418682</td>\n",
       "      <td>-0.143986</td>\n",
       "      <td>0.158732</td>\n",
       "      <td>-0.110476</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.087298</td>\n",
       "      <td>-0.094408</td>\n",
       "      <td>-0.057521</td>\n",
       "      <td>0.017400</td>\n",
       "      <td>0.249831</td>\n",
       "      <td>0.187176</td>\n",
       "      <td>0.209753</td>\n",
       "      <td>0.115751</td>\n",
       "      <td>-0.366013</td>\n",
       "      <td>-0.140920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>252</td>\n",
       "      <td>-0.043379</td>\n",
       "      <td>-0.004612</td>\n",
       "      <td>-0.069868</td>\n",
       "      <td>-0.039644</td>\n",
       "      <td>-0.113420</td>\n",
       "      <td>-0.117409</td>\n",
       "      <td>-0.109536</td>\n",
       "      <td>0.040984</td>\n",
       "      <td>-0.147273</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069852</td>\n",
       "      <td>-0.132072</td>\n",
       "      <td>-0.144296</td>\n",
       "      <td>-0.085587</td>\n",
       "      <td>0.052804</td>\n",
       "      <td>0.003479</td>\n",
       "      <td>0.075373</td>\n",
       "      <td>0.049226</td>\n",
       "      <td>-0.096919</td>\n",
       "      <td>-0.155979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>253</td>\n",
       "      <td>0.443533</td>\n",
       "      <td>0.528896</td>\n",
       "      <td>0.447688</td>\n",
       "      <td>0.966563</td>\n",
       "      <td>0.989150</td>\n",
       "      <td>0.925471</td>\n",
       "      <td>-0.064117</td>\n",
       "      <td>0.476835</td>\n",
       "      <td>-0.064779</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.058895</td>\n",
       "      <td>0.321025</td>\n",
       "      <td>0.264615</td>\n",
       "      <td>0.296434</td>\n",
       "      <td>0.621609</td>\n",
       "      <td>0.593143</td>\n",
       "      <td>0.443181</td>\n",
       "      <td>0.285706</td>\n",
       "      <td>-0.514714</td>\n",
       "      <td>0.033446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>254</td>\n",
       "      <td>0.000453</td>\n",
       "      <td>0.052817</td>\n",
       "      <td>-0.031914</td>\n",
       "      <td>0.062507</td>\n",
       "      <td>0.040163</td>\n",
       "      <td>0.041464</td>\n",
       "      <td>-0.127123</td>\n",
       "      <td>0.092584</td>\n",
       "      <td>-0.142807</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.079904</td>\n",
       "      <td>-0.139591</td>\n",
       "      <td>-0.129375</td>\n",
       "      <td>-0.057819</td>\n",
       "      <td>0.120919</td>\n",
       "      <td>0.034853</td>\n",
       "      <td>0.083771</td>\n",
       "      <td>0.048612</td>\n",
       "      <td>-0.145312</td>\n",
       "      <td>-0.133178</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>255 rows × 18212 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id      A1BG  A1BG-AS1       A2M   A2M-AS1     A2MP1    A4GALT  \\\n",
       "0      0  0.157851  0.135139  0.064579  0.188614  0.246985  0.129225   \n",
       "1      1  0.082970  0.112493  0.016122  0.150402  0.242833  0.162413   \n",
       "2      2  0.086733  0.106412  0.018158  0.150171  0.226735  0.199774   \n",
       "3      3  0.051763  0.067409 -0.025391  0.013669  0.008115  0.049289   \n",
       "4      4  0.054975  0.057908 -0.013780  0.038065  0.045544  0.077910   \n",
       "..   ...       ...       ...       ...       ...       ...       ...   \n",
       "250  250 -0.009482  0.043044 -0.058341 -0.033009 -0.093903 -0.090123   \n",
       "251  251  0.182772  0.117246  0.048263  0.107544  0.247084  0.418682   \n",
       "252  252 -0.043379 -0.004612 -0.069868 -0.039644 -0.113420 -0.117409   \n",
       "253  253  0.443533  0.528896  0.447688  0.966563  0.989150  0.925471   \n",
       "254  254  0.000453  0.052817 -0.031914  0.062507  0.040163  0.041464   \n",
       "\n",
       "         AAAS      AACS     AAGAB  ...      ZUP1      ZW10    ZWILCH  \\\n",
       "0   -0.205016  0.159521 -0.132249  ... -0.120751 -0.119468 -0.055896   \n",
       "1   -0.177303  0.130744 -0.135339  ... -0.110065 -0.138271 -0.091429   \n",
       "2   -0.161684  0.133260 -0.131192  ... -0.103275 -0.134534 -0.091236   \n",
       "3   -0.155272  0.105142 -0.134851  ... -0.089427 -0.113620 -0.090622   \n",
       "4   -0.145470  0.100929 -0.135718  ... -0.094463 -0.127061 -0.106618   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "250 -0.132484  0.072513 -0.140553  ... -0.074358 -0.113390 -0.108952   \n",
       "251 -0.143986  0.158732 -0.110476  ... -0.087298 -0.094408 -0.057521   \n",
       "252 -0.109536  0.040984 -0.147273  ... -0.069852 -0.132072 -0.144296   \n",
       "253 -0.064117  0.476835 -0.064779  ... -0.058895  0.321025  0.264615   \n",
       "254 -0.127123  0.092584 -0.142807  ... -0.079904 -0.139591 -0.129375   \n",
       "\n",
       "        ZWINT      ZXDA      ZXDB      ZXDC    ZYG11B       ZYX     ZZEF1  \n",
       "0   -0.013762  0.252637  0.194140  0.214614  0.097543 -0.469122 -0.143820  \n",
       "1   -0.037659  0.190040  0.114548  0.159004  0.075738 -0.352956 -0.133433  \n",
       "2   -0.031357  0.194296  0.116869  0.160172  0.077250 -0.335472 -0.131992  \n",
       "3   -0.036129  0.144817  0.089914  0.145952  0.108163 -0.242637 -0.141276  \n",
       "4   -0.045987  0.147557  0.083280  0.135690  0.073483 -0.270784 -0.155530  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "250 -0.058431  0.086136  0.043518  0.118689  0.097737 -0.131085 -0.139192  \n",
       "251  0.017400  0.249831  0.187176  0.209753  0.115751 -0.366013 -0.140920  \n",
       "252 -0.085587  0.052804  0.003479  0.075373  0.049226 -0.096919 -0.155979  \n",
       "253  0.296434  0.621609  0.593143  0.443181  0.285706 -0.514714  0.033446  \n",
       "254 -0.057819  0.120919  0.034853  0.083771  0.048612 -0.145312 -0.133178  \n",
       "\n",
       "[255 rows x 18212 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_ = pd.read_csv('data/submission_5.csv')\n",
    "pred_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1BG</th>\n",
       "      <th>A1BG-AS1</th>\n",
       "      <th>A2M</th>\n",
       "      <th>A2M-AS1</th>\n",
       "      <th>A2MP1</th>\n",
       "      <th>A4GALT</th>\n",
       "      <th>AAAS</th>\n",
       "      <th>AACS</th>\n",
       "      <th>AAGAB</th>\n",
       "      <th>AAK1</th>\n",
       "      <th>...</th>\n",
       "      <th>ZUP1</th>\n",
       "      <th>ZW10</th>\n",
       "      <th>ZWILCH</th>\n",
       "      <th>ZWINT</th>\n",
       "      <th>ZXDA</th>\n",
       "      <th>ZXDB</th>\n",
       "      <th>ZXDC</th>\n",
       "      <th>ZYG11B</th>\n",
       "      <th>ZYX</th>\n",
       "      <th>ZZEF1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.218221</td>\n",
       "      <td>0.179731</td>\n",
       "      <td>-0.054798</td>\n",
       "      <td>0.024871</td>\n",
       "      <td>0.493190</td>\n",
       "      <td>0.581067</td>\n",
       "      <td>-0.055398</td>\n",
       "      <td>0.232303</td>\n",
       "      <td>-0.069473</td>\n",
       "      <td>0.001176</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009535</td>\n",
       "      <td>0.052298</td>\n",
       "      <td>-0.105074</td>\n",
       "      <td>0.135251</td>\n",
       "      <td>0.341514</td>\n",
       "      <td>0.257881</td>\n",
       "      <td>0.153122</td>\n",
       "      <td>0.099098</td>\n",
       "      <td>-0.224445</td>\n",
       "      <td>-0.131643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.146980</td>\n",
       "      <td>0.038844</td>\n",
       "      <td>-0.144263</td>\n",
       "      <td>-0.234001</td>\n",
       "      <td>0.115186</td>\n",
       "      <td>0.338515</td>\n",
       "      <td>-0.070514</td>\n",
       "      <td>0.154567</td>\n",
       "      <td>-0.093607</td>\n",
       "      <td>-0.010897</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.112410</td>\n",
       "      <td>-0.008791</td>\n",
       "      <td>-0.160951</td>\n",
       "      <td>0.059692</td>\n",
       "      <td>0.214382</td>\n",
       "      <td>0.146892</td>\n",
       "      <td>0.106424</td>\n",
       "      <td>0.080884</td>\n",
       "      <td>-0.237877</td>\n",
       "      <td>-0.148247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.260201</td>\n",
       "      <td>0.128910</td>\n",
       "      <td>0.008374</td>\n",
       "      <td>-0.068510</td>\n",
       "      <td>0.356836</td>\n",
       "      <td>0.640791</td>\n",
       "      <td>-0.032256</td>\n",
       "      <td>0.253137</td>\n",
       "      <td>-0.039208</td>\n",
       "      <td>0.028447</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034453</td>\n",
       "      <td>0.077636</td>\n",
       "      <td>-0.067954</td>\n",
       "      <td>0.163916</td>\n",
       "      <td>0.350582</td>\n",
       "      <td>0.258399</td>\n",
       "      <td>0.176241</td>\n",
       "      <td>0.123509</td>\n",
       "      <td>-0.196821</td>\n",
       "      <td>-0.107288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.252309</td>\n",
       "      <td>0.121778</td>\n",
       "      <td>-0.002467</td>\n",
       "      <td>-0.081616</td>\n",
       "      <td>0.337699</td>\n",
       "      <td>0.623014</td>\n",
       "      <td>-0.033744</td>\n",
       "      <td>0.247436</td>\n",
       "      <td>-0.040560</td>\n",
       "      <td>0.030770</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.040627</td>\n",
       "      <td>0.072673</td>\n",
       "      <td>-0.075436</td>\n",
       "      <td>0.157450</td>\n",
       "      <td>0.341365</td>\n",
       "      <td>0.250930</td>\n",
       "      <td>0.171957</td>\n",
       "      <td>0.122709</td>\n",
       "      <td>-0.197713</td>\n",
       "      <td>-0.109039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.179937</td>\n",
       "      <td>0.068156</td>\n",
       "      <td>-0.097399</td>\n",
       "      <td>-0.180143</td>\n",
       "      <td>0.193829</td>\n",
       "      <td>0.445006</td>\n",
       "      <td>-0.053160</td>\n",
       "      <td>0.188950</td>\n",
       "      <td>-0.071118</td>\n",
       "      <td>0.021154</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.087039</td>\n",
       "      <td>0.019429</td>\n",
       "      <td>-0.134014</td>\n",
       "      <td>0.092073</td>\n",
       "      <td>0.258866</td>\n",
       "      <td>0.186053</td>\n",
       "      <td>0.130042</td>\n",
       "      <td>0.099978</td>\n",
       "      <td>-0.215229</td>\n",
       "      <td>-0.130735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>0.262735</td>\n",
       "      <td>0.137643</td>\n",
       "      <td>0.010280</td>\n",
       "      <td>-0.052465</td>\n",
       "      <td>0.380265</td>\n",
       "      <td>0.651925</td>\n",
       "      <td>-0.032190</td>\n",
       "      <td>0.257872</td>\n",
       "      <td>-0.038919</td>\n",
       "      <td>0.032956</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026895</td>\n",
       "      <td>0.081294</td>\n",
       "      <td>-0.067910</td>\n",
       "      <td>0.166742</td>\n",
       "      <td>0.360593</td>\n",
       "      <td>0.266977</td>\n",
       "      <td>0.178082</td>\n",
       "      <td>0.123737</td>\n",
       "      <td>-0.195710</td>\n",
       "      <td>-0.107242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>0.229755</td>\n",
       "      <td>0.119831</td>\n",
       "      <td>-0.035955</td>\n",
       "      <td>-0.085194</td>\n",
       "      <td>0.332474</td>\n",
       "      <td>0.576855</td>\n",
       "      <td>-0.044853</td>\n",
       "      <td>0.230195</td>\n",
       "      <td>-0.055966</td>\n",
       "      <td>0.015955</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.042312</td>\n",
       "      <td>0.057204</td>\n",
       "      <td>-0.094581</td>\n",
       "      <td>0.140514</td>\n",
       "      <td>0.324180</td>\n",
       "      <td>0.237511</td>\n",
       "      <td>0.158389</td>\n",
       "      <td>0.110620</td>\n",
       "      <td>-0.211132</td>\n",
       "      <td>-0.120791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>0.219246</td>\n",
       "      <td>0.111294</td>\n",
       "      <td>-0.050836</td>\n",
       "      <td>-0.100878</td>\n",
       "      <td>0.309572</td>\n",
       "      <td>0.547538</td>\n",
       "      <td>-0.047613</td>\n",
       "      <td>0.222076</td>\n",
       "      <td>-0.060479</td>\n",
       "      <td>0.012676</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.049701</td>\n",
       "      <td>0.049368</td>\n",
       "      <td>-0.101029</td>\n",
       "      <td>0.127463</td>\n",
       "      <td>0.312100</td>\n",
       "      <td>0.227682</td>\n",
       "      <td>0.153151</td>\n",
       "      <td>0.106363</td>\n",
       "      <td>-0.215378</td>\n",
       "      <td>-0.124312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>0.266777</td>\n",
       "      <td>0.198003</td>\n",
       "      <td>0.006471</td>\n",
       "      <td>0.058443</td>\n",
       "      <td>0.542213</td>\n",
       "      <td>0.705397</td>\n",
       "      <td>-0.041422</td>\n",
       "      <td>0.273030</td>\n",
       "      <td>-0.048167</td>\n",
       "      <td>0.010054</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025350</td>\n",
       "      <td>0.090661</td>\n",
       "      <td>-0.062945</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>0.396374</td>\n",
       "      <td>0.299574</td>\n",
       "      <td>0.183930</td>\n",
       "      <td>0.114505</td>\n",
       "      <td>-0.214198</td>\n",
       "      <td>-0.116457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>0.202584</td>\n",
       "      <td>0.092384</td>\n",
       "      <td>-0.070552</td>\n",
       "      <td>-0.135625</td>\n",
       "      <td>0.258835</td>\n",
       "      <td>0.504495</td>\n",
       "      <td>-0.053548</td>\n",
       "      <td>0.207348</td>\n",
       "      <td>-0.069096</td>\n",
       "      <td>0.008265</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.066069</td>\n",
       "      <td>0.036302</td>\n",
       "      <td>-0.116292</td>\n",
       "      <td>0.113463</td>\n",
       "      <td>0.290806</td>\n",
       "      <td>0.209000</td>\n",
       "      <td>0.142768</td>\n",
       "      <td>0.100492</td>\n",
       "      <td>-0.219275</td>\n",
       "      <td>-0.129686</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>255 rows × 18211 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         A1BG  A1BG-AS1       A2M   A2M-AS1     A2MP1    A4GALT      AAAS  \\\n",
       "0    0.218221  0.179731 -0.054798  0.024871  0.493190  0.581067 -0.055398   \n",
       "1    0.146980  0.038844 -0.144263 -0.234001  0.115186  0.338515 -0.070514   \n",
       "2    0.260201  0.128910  0.008374 -0.068510  0.356836  0.640791 -0.032256   \n",
       "3    0.252309  0.121778 -0.002467 -0.081616  0.337699  0.623014 -0.033744   \n",
       "4    0.179937  0.068156 -0.097399 -0.180143  0.193829  0.445006 -0.053160   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "250  0.262735  0.137643  0.010280 -0.052465  0.380265  0.651925 -0.032190   \n",
       "251  0.229755  0.119831 -0.035955 -0.085194  0.332474  0.576855 -0.044853   \n",
       "252  0.219246  0.111294 -0.050836 -0.100878  0.309572  0.547538 -0.047613   \n",
       "253  0.266777  0.198003  0.006471  0.058443  0.542213  0.705397 -0.041422   \n",
       "254  0.202584  0.092384 -0.070552 -0.135625  0.258835  0.504495 -0.053548   \n",
       "\n",
       "         AACS     AAGAB      AAK1  ...      ZUP1      ZW10    ZWILCH  \\\n",
       "0    0.232303 -0.069473  0.001176  ...  0.009535  0.052298 -0.105074   \n",
       "1    0.154567 -0.093607 -0.010897  ... -0.112410 -0.008791 -0.160951   \n",
       "2    0.253137 -0.039208  0.028447  ... -0.034453  0.077636 -0.067954   \n",
       "3    0.247436 -0.040560  0.030770  ... -0.040627  0.072673 -0.075436   \n",
       "4    0.188950 -0.071118  0.021154  ... -0.087039  0.019429 -0.134014   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "250  0.257872 -0.038919  0.032956  ... -0.026895  0.081294 -0.067910   \n",
       "251  0.230195 -0.055966  0.015955  ... -0.042312  0.057204 -0.094581   \n",
       "252  0.222076 -0.060479  0.012676  ... -0.049701  0.049368 -0.101029   \n",
       "253  0.273030 -0.048167  0.010054  ...  0.025350  0.090661 -0.062945   \n",
       "254  0.207348 -0.069096  0.008265  ... -0.066069  0.036302 -0.116292   \n",
       "\n",
       "        ZWINT      ZXDA      ZXDB      ZXDC    ZYG11B       ZYX     ZZEF1  \n",
       "0    0.135251  0.341514  0.257881  0.153122  0.099098 -0.224445 -0.131643  \n",
       "1    0.059692  0.214382  0.146892  0.106424  0.080884 -0.237877 -0.148247  \n",
       "2    0.163916  0.350582  0.258399  0.176241  0.123509 -0.196821 -0.107288  \n",
       "3    0.157450  0.341365  0.250930  0.171957  0.122709 -0.197713 -0.109039  \n",
       "4    0.092073  0.258866  0.186053  0.130042  0.099978 -0.215229 -0.130735  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "250  0.166742  0.360593  0.266977  0.178082  0.123737 -0.195710 -0.107242  \n",
       "251  0.140514  0.324180  0.237511  0.158389  0.110620 -0.211132 -0.120791  \n",
       "252  0.127463  0.312100  0.227682  0.153151  0.106363 -0.215378 -0.124312  \n",
       "253  0.179200  0.396374  0.299574  0.183930  0.114505 -0.214198 -0.116457  \n",
       "254  0.113463  0.290806  0.209000  0.142768  0.100492 -0.219275 -0.129686  \n",
       "\n",
       "[255 rows x 18211 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
